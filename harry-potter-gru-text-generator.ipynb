{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTS\nimport psutil\nimport re\nimport os\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.keras.utils import Sequence\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nfrom keras import backend as K\nimport math\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:31:09.090075Z","iopub.execute_input":"2022-09-07T18:31:09.090491Z","iopub.status.idle":"2022-09-07T18:31:21.509511Z","shell.execute_reply.started":"2022-09-07T18:31:09.090389Z","shell.execute_reply":"2022-09-07T18:31:21.508455Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#DIRECTORIES\nDATASET = \"../input/harry-potter-gru-text-generator\"\nDATA_PATH = \"../input/harry-potter-philosophers-stone-preprocessed/Harry_Potter_philosophers_stone.txt\"\nSAVED_MODEL_PATH = \"../input/harry-potter-gru-text-generator/Best_weights.hdf5\"\nCHECKPOINT_PATH = \"Best_weights.hdf5\"","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:31:21.511437Z","iopub.execute_input":"2022-09-07T18:31:21.511745Z","iopub.status.idle":"2022-09-07T18:31:21.517557Z","shell.execute_reply.started":"2022-09-07T18:31:21.511708Z","shell.execute_reply":"2022-09-07T18:31:21.516529Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Copy file from Input to Output(to easier create a new dataset with updated weights)\nfor file in os.listdir(DATASET):\n    if file.endswith('hdf5') == False:\n        path = os.path.join(DATASET, file)\n        !cp -r $path ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the file\ntext = open(DATA_PATH, \"r\", encoding=\"utf-8\").read().lower()\nwords = text.split()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:31:21.519044Z","iopub.execute_input":"2022-09-07T18:31:21.519393Z","iopub.status.idle":"2022-09-07T18:31:21.551961Z","shell.execute_reply.started":"2022-09-07T18:31:21.519345Z","shell.execute_reply":"2022-09-07T18:31:21.550823Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#How many unique words\nwords_unique = Counter(words).most_common()\ndictionary = {}\nfor word in words_unique:\n    dictionary[word[0]] = word[1]\ndict_values = list(dictionary.values())","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:50:58.556864Z","iopub.execute_input":"2022-09-07T18:50:58.557490Z","iopub.status.idle":"2022-09-07T18:50:58.565208Z","shell.execute_reply.started":"2022-09-07T18:50:58.557452Z","shell.execute_reply":"2022-09-07T18:50:58.564252Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#trying to remove sentences with low usage words.\nwords_remove = [list(dictionary.keys())[idx] for idx, val in enumerate(dict_values) if val <= 3 and len(list(dictionary.keys())[idx]) >= 3]\nprint(len(words_remove))\nsentences = re.split('[.!?]', text)\nsent = []\nsent = [re.sub('[\\n]', '', sentence) for sentence in sentences]\nsentences = [sentence for sentence in sent if not any(word in sentence for word in words_remove)]\nprint(len(sentences))","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:48:53.710020Z","iopub.execute_input":"2022-09-07T18:48:53.710546Z","iopub.status.idle":"2022-09-07T18:48:54.667408Z","shell.execute_reply.started":"2022-09-07T18:48:53.710487Z","shell.execute_reply":"2022-09-07T18:48:54.666507Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"3938\n","output_type":"stream"}]},{"cell_type":"code","source":"#text preprocessing\nendings  = ('.', '!', '?')\n\nfor idx, word in enumerate(words):\n    if word.endswith(endings) and word not in endings:\n        words[idx] = re.sub('[.!?]', '', word)\n        words.insert(idx+1, word[-1])\n    if words[idx].startswith('.') and word not in endings:\n        words[idx] = re.sub('[.]', '', word)\n        words.insert(idx-1, '.')\n    if re.search('.[.].', words[idx]):\n        w = word.split('.')\n        words[idx] = '.'\n        words.insert(idx-1, w[0])\n        words.insert(idx+1, w[-1])\ntext = ' '.join(words)\n\n#How many unique words\nwords_unique = Counter(words).most_common()\ndictionary = {}\nfor word in words_unique:\n    dictionary[word[0]] = word[1]\ndict_values = list(dictionary.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_text = ''.join(sentences)\nnew_text = re.sub('  ', ' ', new_text)\nwords = new_text.split()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:50:45.002681Z","iopub.execute_input":"2022-09-07T18:50:45.002997Z","iopub.status.idle":"2022-09-07T18:50:45.010628Z","shell.execute_reply.started":"2022-09-07T18:50:45.002960Z","shell.execute_reply":"2022-09-07T18:50:45.009665Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"print(len(words_unique))\nplt.figure(figsize=(10,10))\nplt.plot(dict_values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:51:02.242258Z","iopub.execute_input":"2022-09-07T18:51:02.242980Z","iopub.status.idle":"2022-09-07T18:51:02.509220Z","shell.execute_reply.started":"2022-09-07T18:51:02.242928Z","shell.execute_reply":"2022-09-07T18:51:02.508403Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"979\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#DATA PREPERATION\nLEN_SEQUENCE = 3\n\n#Creating generator\nclass data_sequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n\n        return batch_x, batch_y\n\n#OPEN DATA\n#text = open(DATA_PATH, \"r\", encoding=\"utf-8\").read().lower()\n\n#DATA PREPARATION\nWORDS_IN_TEXT = len(words)\nALL_WORDS = sorted(set(words))\n\nword_to_int = {word:i for i, word in enumerate(ALL_WORDS)}\nint_to_word = {v: k for k, v in word_to_int.items()}\n\nX = []\ny = []\n\nperc = [per for per in range(10, 101, 10)]\nj = 0\n\nfor i in range(0, WORDS_IN_TEXT-LEN_SEQUENCE, 1):\n#     if i == int((WORDS_IN_TEXT-LEN_SEQUENCE-1) * perc[j] / 100):\n#         print(f'{perc[j]}%')\n#         j += 1\n#         process = psutil.Process(os.getpid())\n#         print(process.memory_info().rss / 2**20)  # in bytes \n    sequence_X = words[i:i+LEN_SEQUENCE]\n    sequence_y = words[i+LEN_SEQUENCE]\n    X.append([word_to_int[c] for c in sequence_X])\n    y.append(word_to_int[sequence_y])   \n\npatterns = len(X)\nX_data = np.reshape(X, (patterns, LEN_SEQUENCE, 1))\n# print('X_data_reshape')\n# process = psutil.Process(os.getpid())\n# print(process.memory_info().rss / 2**20)  # in bytes \n# X_data = X_data.astype(\"float32\")\n# print('X_data_float32')\n# process = psutil.Process(os.getpid())\n# print(process.memory_info().rss / 2**20)  # in bytes \n# X_data = X_data/float(len(ALL_WORDS))\n# print('preproces')\n# process = psutil.Process(os.getpid())\n# print(process.memory_info().rss / 2**20)  # in bytes \ny_data = np_utils.to_categorical(y)\n\n#Data split\nval_samples = int(patterns * 0.80)\nX_train = X_data[:val_samples, :, :]\nX_test = X_data[val_samples:, :, :]\nY_train = y_data[:val_samples]\nY_test = y_data[val_samples:]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:51:18.313217Z","iopub.execute_input":"2022-09-07T18:51:18.313897Z","iopub.status.idle":"2022-09-07T18:51:18.364331Z","shell.execute_reply.started":"2022-09-07T18:51:18.313842Z","shell.execute_reply":"2022-09-07T18:51:18.363577Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#TRAINING PIPELINE (from weights)\n\n#PARAMETERS\nUNITS = 512\nDROPOUT = 0.2\nBATCH = 64\nEPOCH = 1\n\n#CHECKPOINT INITIALIZATION\ncheckpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\n#STOP TRAINING IF NO IMPROVE\n#early_stop = tf.keras.callbacks.EarlyStopping(\"loss\", patience=2, restore_best_weights=True)\n\n#LOAD WEIGHTS(from dataset)\nsequence = data_sequence(X_data, y_data, BATCH)\nmodel = keras.models.load_model(SAVED_MODEL_PATH)\ntensorboard_log = tf.keras.callbacks.TensorBoard(f\"./logs/{datetime.datetime.now().strftime('%d-%m-%y-%H:%M')}-L:{LEN_SEQUENCE}_U:{UNITS}_D:{DROPOUT}_B:{BATCH}\", histogram_freq=1)\n\n#CHANGE lr\nK.set_value(model.optimizer.learning_rate, 0.0001)\n\n# START TREINING\nmodel.fit(sequence, epochs=EPOCH, callbacks=[checkpoint, tensorboard_log])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRAIN FROM SCRATCH\n\n#PARAMETERS\nUNITS = 512\nDROPOUT = 0.5\nBATCH = 64\nEPOCH = 100\n\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(GRU(UNITS, input_shape=(X_data.shape[1], X_data.shape[2]), return_sequences=True))\nmodel.add(Dropout(DROPOUT))\nmodel.add(GRU(UNITS))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(y_data.shape[1], activation=\"softmax\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n\n#CHECKPOINT INITIALIZATION\ncheckpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ntrain_sequence = data_sequence(X_train, Y_train, BATCH)\ntest_sequence = data_sequence(X_test, Y_test, BATCH)\ntensorboard_log = tf.keras.callbacks.TensorBoard(f\"./logs/{datetime.datetime.now().strftime('%d-%m-%y-%H:%M')}-L:{LEN_SEQUENCE}_U:{UNITS}_D:{DROPOUT}_B:{BATCH}\", histogram_freq=1)\nmodel.fit(train_sequence, epochs=EPOCH, callbacks=[checkpoint, tensorboard_log], validation_data=test_sequence)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T19:12:23.236852Z","iopub.execute_input":"2022-09-07T19:12:23.237201Z","iopub.status.idle":"2022-09-07T19:23:29.633065Z","shell.execute_reply.started":"2022-09-07T19:12:23.237168Z","shell.execute_reply":"2022-09-07T19:23:29.632361Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"2022-09-07 19:12:23.947109: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2022-09-07 19:12:23.947329: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2022-09-07 19:12:23.947940: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n 2/92 [..............................] - ETA: 16s - loss: 6.8513 ","output_type":"stream"},{"name":"stderr","text":"2022-09-07 19:12:28.796192: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2022-09-07 19:12:28.796423: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2022-09-07 19:12:28.921139: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2022-09-07 19:12:28.928923: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2022-09-07 19:12:28.937877: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28\n\n2022-09-07 19:12:28.944457: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.trace.json.gz\n2022-09-07 19:12:28.959361: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28\n\n2022-09-07 19:12:28.963773: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.memory_profile.json.gz\n2022-09-07 19:12:28.964709: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28\nDumped tool data for xplane.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.xplane.pb\nDumped tool data for overview_page.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.overview_page.pb\nDumped tool data for input_pipeline.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_12_28/bfdca412e011.kernel_stats.pb\n\n","output_type":"stream"},{"name":"stdout","text":"92/92 [==============================] - 12s 80ms/step - loss: 6.1586 - val_loss: 5.8335\n\nEpoch 00001: loss improved from inf to 6.15863, saving model to Best_weights.hdf5\nEpoch 2/100\n92/92 [==============================] - 6s 62ms/step - loss: 5.8809 - val_loss: 5.8662\n\nEpoch 00002: loss improved from 6.15863 to 5.88092, saving model to Best_weights.hdf5\nEpoch 3/100\n92/92 [==============================] - 5s 58ms/step - loss: 5.8230 - val_loss: 5.8801\n\nEpoch 00003: loss improved from 5.88092 to 5.82303, saving model to Best_weights.hdf5\nEpoch 4/100\n92/92 [==============================] - 5s 59ms/step - loss: 5.7845 - val_loss: 5.9240\n\nEpoch 00004: loss improved from 5.82303 to 5.78454, saving model to Best_weights.hdf5\nEpoch 5/100\n92/92 [==============================] - 6s 61ms/step - loss: 5.7522 - val_loss: 5.8988\n\nEpoch 00005: loss improved from 5.78454 to 5.75215, saving model to Best_weights.hdf5\nEpoch 6/100\n92/92 [==============================] - 6s 66ms/step - loss: 5.7229 - val_loss: 5.9361\n\nEpoch 00006: loss improved from 5.75215 to 5.72294, saving model to Best_weights.hdf5\nEpoch 7/100\n92/92 [==============================] - 5s 56ms/step - loss: 5.7022 - val_loss: 5.9634\n\nEpoch 00007: loss improved from 5.72294 to 5.70223, saving model to Best_weights.hdf5\nEpoch 8/100\n92/92 [==============================] - 5s 57ms/step - loss: 5.6887 - val_loss: 5.9608\n\nEpoch 00008: loss improved from 5.70223 to 5.68869, saving model to Best_weights.hdf5\nEpoch 9/100\n92/92 [==============================] - 5s 55ms/step - loss: 5.6506 - val_loss: 5.9857\n\nEpoch 00009: loss improved from 5.68869 to 5.65055, saving model to Best_weights.hdf5\nEpoch 10/100\n92/92 [==============================] - 5s 58ms/step - loss: 5.6204 - val_loss: 5.9952\n\nEpoch 00010: loss improved from 5.65055 to 5.62041, saving model to Best_weights.hdf5\nEpoch 11/100\n92/92 [==============================] - 6s 63ms/step - loss: 5.6049 - val_loss: 6.0231\n\nEpoch 00011: loss improved from 5.62041 to 5.60488, saving model to Best_weights.hdf5\nEpoch 12/100\n92/92 [==============================] - 6s 62ms/step - loss: 5.5801 - val_loss: 6.0452\n\nEpoch 00012: loss improved from 5.60488 to 5.58015, saving model to Best_weights.hdf5\nEpoch 13/100\n92/92 [==============================] - 6s 66ms/step - loss: 5.5511 - val_loss: 6.0373\n\nEpoch 00013: loss improved from 5.58015 to 5.55111, saving model to Best_weights.hdf5\nEpoch 14/100\n92/92 [==============================] - 5s 59ms/step - loss: 5.5209 - val_loss: 6.0662\n\nEpoch 00014: loss improved from 5.55111 to 5.52089, saving model to Best_weights.hdf5\nEpoch 15/100\n92/92 [==============================] - 6s 63ms/step - loss: 5.4930 - val_loss: 6.0952\n\nEpoch 00015: loss improved from 5.52089 to 5.49303, saving model to Best_weights.hdf5\nEpoch 16/100\n92/92 [==============================] - 6s 70ms/step - loss: 5.4749 - val_loss: 6.0793\n\nEpoch 00016: loss improved from 5.49303 to 5.47491, saving model to Best_weights.hdf5\nEpoch 17/100\n92/92 [==============================] - 6s 65ms/step - loss: 5.4607 - val_loss: 6.1297\n\nEpoch 00017: loss improved from 5.47491 to 5.46069, saving model to Best_weights.hdf5\nEpoch 18/100\n92/92 [==============================] - 6s 65ms/step - loss: 5.4263 - val_loss: 6.1237\n\nEpoch 00018: loss improved from 5.46069 to 5.42629, saving model to Best_weights.hdf5\nEpoch 19/100\n92/92 [==============================] - 6s 64ms/step - loss: 5.3971 - val_loss: 6.1402\n\nEpoch 00019: loss improved from 5.42629 to 5.39708, saving model to Best_weights.hdf5\nEpoch 20/100\n92/92 [==============================] - 6s 67ms/step - loss: 5.3828 - val_loss: 6.1582\n\nEpoch 00020: loss improved from 5.39708 to 5.38285, saving model to Best_weights.hdf5\nEpoch 21/100\n92/92 [==============================] - 6s 70ms/step - loss: 5.3553 - val_loss: 6.1769\n\nEpoch 00021: loss improved from 5.38285 to 5.35533, saving model to Best_weights.hdf5\nEpoch 22/100\n92/92 [==============================] - 6s 63ms/step - loss: 5.3309 - val_loss: 6.2066\n\nEpoch 00022: loss improved from 5.35533 to 5.33087, saving model to Best_weights.hdf5\nEpoch 23/100\n92/92 [==============================] - 5s 57ms/step - loss: 5.3087 - val_loss: 6.2039\n\nEpoch 00023: loss improved from 5.33087 to 5.30870, saving model to Best_weights.hdf5\nEpoch 24/100\n92/92 [==============================] - 6s 63ms/step - loss: 5.2939 - val_loss: 6.2154\n\nEpoch 00024: loss improved from 5.30870 to 5.29391, saving model to Best_weights.hdf5\nEpoch 25/100\n92/92 [==============================] - 6s 61ms/step - loss: 5.2759 - val_loss: 6.2324\n\nEpoch 00025: loss improved from 5.29391 to 5.27595, saving model to Best_weights.hdf5\nEpoch 26/100\n92/92 [==============================] - 6s 68ms/step - loss: 5.2666 - val_loss: 6.2597\n\nEpoch 00026: loss improved from 5.27595 to 5.26664, saving model to Best_weights.hdf5\nEpoch 27/100\n92/92 [==============================] - 6s 63ms/step - loss: 5.2287 - val_loss: 6.2645\n\nEpoch 00027: loss improved from 5.26664 to 5.22874, saving model to Best_weights.hdf5\nEpoch 28/100\n92/92 [==============================] - 6s 62ms/step - loss: 5.1958 - val_loss: 6.2873\n\nEpoch 00028: loss improved from 5.22874 to 5.19579, saving model to Best_weights.hdf5\nEpoch 29/100\n92/92 [==============================] - 6s 60ms/step - loss: 5.1752 - val_loss: 6.3188\n\nEpoch 00029: loss improved from 5.19579 to 5.17518, saving model to Best_weights.hdf5\nEpoch 30/100\n92/92 [==============================] - 6s 60ms/step - loss: 5.1574 - val_loss: 6.3128\n\nEpoch 00030: loss improved from 5.17518 to 5.15745, saving model to Best_weights.hdf5\nEpoch 31/100\n92/92 [==============================] - 6s 70ms/step - loss: 5.1378 - val_loss: 6.3271\n\nEpoch 00031: loss improved from 5.15745 to 5.13783, saving model to Best_weights.hdf5\nEpoch 32/100\n92/92 [==============================] - 6s 60ms/step - loss: 5.1221 - val_loss: 6.3346\n\nEpoch 00032: loss improved from 5.13783 to 5.12212, saving model to Best_weights.hdf5\nEpoch 33/100\n92/92 [==============================] - 6s 61ms/step - loss: 5.1158 - val_loss: 6.3827\n\nEpoch 00033: loss improved from 5.12212 to 5.11579, saving model to Best_weights.hdf5\nEpoch 34/100\n92/92 [==============================] - 6s 61ms/step - loss: 5.0794 - val_loss: 6.4132\n\nEpoch 00034: loss improved from 5.11579 to 5.07944, saving model to Best_weights.hdf5\nEpoch 35/100\n92/92 [==============================] - 6s 70ms/step - loss: 5.0626 - val_loss: 6.4350\n\nEpoch 00035: loss improved from 5.07944 to 5.06258, saving model to Best_weights.hdf5\nEpoch 36/100\n92/92 [==============================] - 5s 57ms/step - loss: 5.0302 - val_loss: 6.4301\n\nEpoch 00036: loss improved from 5.06258 to 5.03017, saving model to Best_weights.hdf5\nEpoch 37/100\n92/92 [==============================] - 5s 59ms/step - loss: 5.0174 - val_loss: 6.4609\n\nEpoch 00037: loss improved from 5.03017 to 5.01736, saving model to Best_weights.hdf5\nEpoch 38/100\n92/92 [==============================] - 5s 58ms/step - loss: 5.0046 - val_loss: 6.4834\n\nEpoch 00038: loss improved from 5.01736 to 5.00459, saving model to Best_weights.hdf5\nEpoch 39/100\n92/92 [==============================] - 5s 59ms/step - loss: 4.9754 - val_loss: 6.4882\n\nEpoch 00039: loss improved from 5.00459 to 4.97540, saving model to Best_weights.hdf5\nEpoch 40/100\n92/92 [==============================] - 6s 66ms/step - loss: 4.9589 - val_loss: 6.5291\n\nEpoch 00040: loss improved from 4.97540 to 4.95889, saving model to Best_weights.hdf5\nEpoch 41/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.9564 - val_loss: 6.5159\n\nEpoch 00041: loss improved from 4.95889 to 4.95640, saving model to Best_weights.hdf5\nEpoch 42/100\n92/92 [==============================] - 5s 59ms/step - loss: 4.9299 - val_loss: 6.5494\n\nEpoch 00042: loss improved from 4.95640 to 4.92987, saving model to Best_weights.hdf5\nEpoch 43/100\n92/92 [==============================] - 6s 61ms/step - loss: 4.9004 - val_loss: 6.5593\n\nEpoch 00043: loss improved from 4.92987 to 4.90042, saving model to Best_weights.hdf5\nEpoch 44/100\n92/92 [==============================] - 6s 61ms/step - loss: 4.8937 - val_loss: 6.5963\n\nEpoch 00044: loss improved from 4.90042 to 4.89374, saving model to Best_weights.hdf5\nEpoch 45/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.8748 - val_loss: 6.6143\n\nEpoch 00045: loss improved from 4.89374 to 4.87478, saving model to Best_weights.hdf5\nEpoch 46/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.8522 - val_loss: 6.6251\n\nEpoch 00046: loss improved from 4.87478 to 4.85217, saving model to Best_weights.hdf5\nEpoch 47/100\n92/92 [==============================] - 6s 60ms/step - loss: 4.8576 - val_loss: 6.6407\n\nEpoch 00047: loss did not improve from 4.85217\nEpoch 48/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.8519 - val_loss: 6.6740\n\nEpoch 00048: loss improved from 4.85217 to 4.85192, saving model to Best_weights.hdf5\nEpoch 49/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.8202 - val_loss: 6.6570\n\nEpoch 00049: loss improved from 4.85192 to 4.82018, saving model to Best_weights.hdf5\nEpoch 50/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.8041 - val_loss: 6.7031\n\nEpoch 00050: loss improved from 4.82018 to 4.80413, saving model to Best_weights.hdf5\nEpoch 51/100\n92/92 [==============================] - 5s 59ms/step - loss: 4.7848 - val_loss: 6.7340\n\nEpoch 00051: loss improved from 4.80413 to 4.78477, saving model to Best_weights.hdf5\nEpoch 52/100\n92/92 [==============================] - 5s 59ms/step - loss: 4.7570 - val_loss: 6.7374\n\nEpoch 00052: loss improved from 4.78477 to 4.75696, saving model to Best_weights.hdf5\nEpoch 53/100\n92/92 [==============================] - 6s 64ms/step - loss: 4.7455 - val_loss: 6.7530\n\nEpoch 00053: loss improved from 4.75696 to 4.74546, saving model to Best_weights.hdf5\nEpoch 54/100\n92/92 [==============================] - 7s 71ms/step - loss: 4.7158 - val_loss: 6.7750\n\nEpoch 00054: loss improved from 4.74546 to 4.71585, saving model to Best_weights.hdf5\nEpoch 55/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.7150 - val_loss: 6.7906\n\nEpoch 00055: loss improved from 4.71585 to 4.71499, saving model to Best_weights.hdf5\nEpoch 56/100\n92/92 [==============================] - 6s 64ms/step - loss: 4.7196 - val_loss: 6.8190\n\nEpoch 00056: loss did not improve from 4.71499\nEpoch 57/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.6967 - val_loss: 6.8401\n\nEpoch 00057: loss improved from 4.71499 to 4.69667, saving model to Best_weights.hdf5\nEpoch 58/100\n92/92 [==============================] - 6s 64ms/step - loss: 4.6651 - val_loss: 6.8518\n\nEpoch 00058: loss improved from 4.69667 to 4.66506, saving model to Best_weights.hdf5\nEpoch 59/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.6409 - val_loss: 6.8584\n\nEpoch 00059: loss improved from 4.66506 to 4.64090, saving model to Best_weights.hdf5\nEpoch 60/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.6217 - val_loss: 6.8760\n\nEpoch 00060: loss improved from 4.64090 to 4.62170, saving model to Best_weights.hdf5\nEpoch 61/100\n92/92 [==============================] - 6s 60ms/step - loss: 4.6366 - val_loss: 6.8791\n\nEpoch 00061: loss did not improve from 4.62170\nEpoch 62/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.6270 - val_loss: 6.9096\n\nEpoch 00062: loss did not improve from 4.62170\nEpoch 63/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.5937 - val_loss: 6.9104\n\nEpoch 00063: loss improved from 4.62170 to 4.59374, saving model to Best_weights.hdf5\nEpoch 64/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.5872 - val_loss: 6.9436\n\nEpoch 00064: loss improved from 4.59374 to 4.58716, saving model to Best_weights.hdf5\nEpoch 65/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.5715 - val_loss: 6.9646\n\nEpoch 00065: loss improved from 4.58716 to 4.57150, saving model to Best_weights.hdf5\nEpoch 66/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.5470 - val_loss: 6.9611\n\nEpoch 00066: loss improved from 4.57150 to 4.54701, saving model to Best_weights.hdf5\nEpoch 67/100\n92/92 [==============================] - 5s 60ms/step - loss: 4.5458 - val_loss: 6.9956\n\nEpoch 00067: loss improved from 4.54701 to 4.54581, saving model to Best_weights.hdf5\nEpoch 68/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.5280 - val_loss: 7.0267\n\nEpoch 00068: loss improved from 4.54581 to 4.52800, saving model to Best_weights.hdf5\nEpoch 69/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.5079 - val_loss: 7.0234\n\nEpoch 00069: loss improved from 4.52800 to 4.50787, saving model to Best_weights.hdf5\nEpoch 70/100\n92/92 [==============================] - 5s 55ms/step - loss: 4.5071 - val_loss: 7.0330\n\nEpoch 00070: loss improved from 4.50787 to 4.50712, saving model to Best_weights.hdf5\nEpoch 71/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.4952 - val_loss: 7.0581\n\nEpoch 00071: loss improved from 4.50712 to 4.49522, saving model to Best_weights.hdf5\nEpoch 72/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.4833 - val_loss: 7.0898\n\nEpoch 00072: loss improved from 4.49522 to 4.48333, saving model to Best_weights.hdf5\nEpoch 73/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.4608 - val_loss: 7.1058\n\nEpoch 00073: loss improved from 4.48333 to 4.46081, saving model to Best_weights.hdf5\nEpoch 74/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.4497 - val_loss: 7.1056\n\nEpoch 00074: loss improved from 4.46081 to 4.44974, saving model to Best_weights.hdf5\nEpoch 75/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.4235 - val_loss: 7.1458\n\nEpoch 00075: loss improved from 4.44974 to 4.42350, saving model to Best_weights.hdf5\nEpoch 76/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.4213 - val_loss: 7.1327\n\nEpoch 00076: loss improved from 4.42350 to 4.42131, saving model to Best_weights.hdf5\nEpoch 77/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.3917 - val_loss: 7.1613\n\nEpoch 00077: loss improved from 4.42131 to 4.39166, saving model to Best_weights.hdf5\nEpoch 78/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.4071 - val_loss: 7.1749\n\nEpoch 00078: loss did not improve from 4.39166\nEpoch 79/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.3803 - val_loss: 7.1898\n\nEpoch 00079: loss improved from 4.39166 to 4.38033, saving model to Best_weights.hdf5\nEpoch 80/100\n92/92 [==============================] - 5s 55ms/step - loss: 4.3796 - val_loss: 7.2162\n\nEpoch 00080: loss improved from 4.38033 to 4.37957, saving model to Best_weights.hdf5\nEpoch 81/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.3646 - val_loss: 7.2303\n\nEpoch 00081: loss improved from 4.37957 to 4.36455, saving model to Best_weights.hdf5\nEpoch 82/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.3485 - val_loss: 7.2465\n\nEpoch 00082: loss improved from 4.36455 to 4.34851, saving model to Best_weights.hdf5\nEpoch 83/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.3445 - val_loss: 7.2872\n\nEpoch 00083: loss improved from 4.34851 to 4.34447, saving model to Best_weights.hdf5\nEpoch 84/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.3040 - val_loss: 7.2829\n\nEpoch 00084: loss improved from 4.34447 to 4.30395, saving model to Best_weights.hdf5\nEpoch 85/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.3260 - val_loss: 7.2782\n\nEpoch 00085: loss did not improve from 4.30395\nEpoch 86/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.3064 - val_loss: 7.3084\n\nEpoch 00086: loss did not improve from 4.30395\nEpoch 87/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.2671 - val_loss: 7.3569\n\nEpoch 00087: loss improved from 4.30395 to 4.26710, saving model to Best_weights.hdf5\nEpoch 88/100\n92/92 [==============================] - 6s 64ms/step - loss: 4.2728 - val_loss: 7.3542\n\nEpoch 00088: loss did not improve from 4.26710\nEpoch 89/100\n92/92 [==============================] - 6s 60ms/step - loss: 4.2719 - val_loss: 7.3750\n\nEpoch 00089: loss did not improve from 4.26710\nEpoch 90/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.2599 - val_loss: 7.3619\n\nEpoch 00090: loss improved from 4.26710 to 4.25992, saving model to Best_weights.hdf5\nEpoch 91/100\n92/92 [==============================] - 6s 61ms/step - loss: 4.2485 - val_loss: 7.3695\n\nEpoch 00091: loss improved from 4.25992 to 4.24854, saving model to Best_weights.hdf5\nEpoch 92/100\n92/92 [==============================] - 6s 67ms/step - loss: 4.2423 - val_loss: 7.4382\n\nEpoch 00092: loss improved from 4.24854 to 4.24232, saving model to Best_weights.hdf5\nEpoch 93/100\n92/92 [==============================] - 6s 64ms/step - loss: 4.2258 - val_loss: 7.4389\n\nEpoch 00093: loss improved from 4.24232 to 4.22581, saving model to Best_weights.hdf5\nEpoch 94/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.2152 - val_loss: 7.4588\n\nEpoch 00094: loss improved from 4.22581 to 4.21516, saving model to Best_weights.hdf5\nEpoch 95/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.2169 - val_loss: 7.4773\n\nEpoch 00095: loss did not improve from 4.21516\nEpoch 96/100\n92/92 [==============================] - 6s 65ms/step - loss: 4.1927 - val_loss: 7.4764\n\nEpoch 00096: loss improved from 4.21516 to 4.19274, saving model to Best_weights.hdf5\nEpoch 97/100\n92/92 [==============================] - 6s 69ms/step - loss: 4.1896 - val_loss: 7.5056\n\nEpoch 00097: loss improved from 4.19274 to 4.18961, saving model to Best_weights.hdf5\nEpoch 98/100\n92/92 [==============================] - 6s 61ms/step - loss: 4.1852 - val_loss: 7.5162\n\nEpoch 00098: loss improved from 4.18961 to 4.18522, saving model to Best_weights.hdf5\nEpoch 99/100\n92/92 [==============================] - 6s 63ms/step - loss: 4.1466 - val_loss: 7.5201\n\nEpoch 00099: loss improved from 4.18522 to 4.14662, saving model to Best_weights.hdf5\nEpoch 100/100\n92/92 [==============================] - 6s 62ms/step - loss: 4.1322 - val_loss: 7.5572\n\nEpoch 00100: loss improved from 4.14662 to 4.13221, saving model to Best_weights.hdf5\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f83f0e2bc10>"},"metadata":{}}]},{"cell_type":"code","source":"model.fit(train_sequence, epochs=100, callbacks=[checkpoint, tensorboard_log], validation_data=test_sequence)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T19:32:07.122925Z","iopub.execute_input":"2022-09-07T19:32:07.123589Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n 2/92 [..............................] - ETA: 16s - loss: 4.1291","output_type":"stream"},{"name":"stderr","text":"2022-09-07 19:32:07.388192: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2022-09-07 19:32:07.388446: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2022-09-07 19:32:07.504756: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2022-09-07 19:32:07.510449: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2022-09-07 19:32:07.516541: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07\n\n2022-09-07 19:32:07.519810: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.trace.json.gz\n2022-09-07 19:32:07.533726: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07\n\n2022-09-07 19:32:07.538593: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.memory_profile.json.gz\n2022-09-07 19:32:07.539552: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07\nDumped tool data for xplane.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.xplane.pb\nDumped tool data for overview_page.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.overview_page.pb\nDumped tool data for input_pipeline.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./logs/07-09-22-19:12-L:3_U:512_D:0.5_B:64/train/plugins/profile/2022_09_07_19_32_07/bfdca412e011.kernel_stats.pb\n\n","output_type":"stream"},{"name":"stdout","text":"92/92 [==============================] - 6s 65ms/step - loss: 4.1442 - val_loss: 7.5573\n\nEpoch 00001: loss did not improve from 4.13221\nEpoch 2/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.1400 - val_loss: 7.5832\n\nEpoch 00002: loss did not improve from 4.13221\nEpoch 3/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.1299 - val_loss: 7.5785\n\nEpoch 00003: loss improved from 4.13221 to 4.12987, saving model to Best_weights.hdf5\nEpoch 4/100\n92/92 [==============================] - 6s 65ms/step - loss: 4.1226 - val_loss: 7.6347\n\nEpoch 00004: loss improved from 4.12987 to 4.12261, saving model to Best_weights.hdf5\nEpoch 5/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.1228 - val_loss: 7.6493\n\nEpoch 00005: loss did not improve from 4.12261\nEpoch 6/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.0661 - val_loss: 7.6465\n\nEpoch 00006: loss improved from 4.12261 to 4.06615, saving model to Best_weights.hdf5\nEpoch 7/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.0876 - val_loss: 7.6562\n\nEpoch 00007: loss did not improve from 4.06615\nEpoch 8/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.0910 - val_loss: 7.6892\n\nEpoch 00008: loss did not improve from 4.06615\nEpoch 9/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.0874 - val_loss: 7.6979\n\nEpoch 00009: loss did not improve from 4.06615\nEpoch 10/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.0673 - val_loss: 7.7119\n\nEpoch 00010: loss did not improve from 4.06615\nEpoch 11/100\n92/92 [==============================] - 5s 55ms/step - loss: 4.0736 - val_loss: 7.7410\n\nEpoch 00011: loss did not improve from 4.06615\nEpoch 12/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.0458 - val_loss: 7.7550\n\nEpoch 00012: loss improved from 4.06615 to 4.04576, saving model to Best_weights.hdf5\nEpoch 13/100\n92/92 [==============================] - 6s 66ms/step - loss: 4.0165 - val_loss: 7.7785\n\nEpoch 00013: loss improved from 4.04576 to 4.01650, saving model to Best_weights.hdf5\nEpoch 14/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9937 - val_loss: 7.7925\n\nEpoch 00014: loss improved from 4.01650 to 3.99371, saving model to Best_weights.hdf5\nEpoch 15/100\n92/92 [==============================] - 5s 58ms/step - loss: 4.0396 - val_loss: 7.8103\n\nEpoch 00015: loss did not improve from 3.99371\nEpoch 16/100\n92/92 [==============================] - 5s 56ms/step - loss: 4.0159 - val_loss: 7.7877\n\nEpoch 00016: loss did not improve from 3.99371\nEpoch 17/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9986 - val_loss: 7.8322\n\nEpoch 00017: loss did not improve from 3.99371\nEpoch 18/100\n92/92 [==============================] - 5s 59ms/step - loss: 3.9885 - val_loss: 7.8442\n\nEpoch 00018: loss improved from 3.99371 to 3.98851, saving model to Best_weights.hdf5\nEpoch 19/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9823 - val_loss: 7.8699\n\nEpoch 00019: loss improved from 3.98851 to 3.98234, saving model to Best_weights.hdf5\nEpoch 20/100\n92/92 [==============================] - 5s 57ms/step - loss: 4.0055 - val_loss: 7.8394\n\nEpoch 00020: loss did not improve from 3.98234\nEpoch 21/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9564 - val_loss: 7.9138\n\nEpoch 00021: loss improved from 3.98234 to 3.95644, saving model to Best_weights.hdf5\nEpoch 22/100\n92/92 [==============================] - 6s 66ms/step - loss: 3.9644 - val_loss: 7.9247\n\nEpoch 00022: loss did not improve from 3.95644\nEpoch 23/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9689 - val_loss: 7.9287\n\nEpoch 00023: loss did not improve from 3.95644\nEpoch 24/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9426 - val_loss: 7.9041\n\nEpoch 00024: loss improved from 3.95644 to 3.94264, saving model to Best_weights.hdf5\nEpoch 25/100\n92/92 [==============================] - 5s 55ms/step - loss: 3.9302 - val_loss: 7.9586\n\nEpoch 00025: loss improved from 3.94264 to 3.93023, saving model to Best_weights.hdf5\nEpoch 26/100\n92/92 [==============================] - 6s 64ms/step - loss: 3.9261 - val_loss: 7.9553\n\nEpoch 00026: loss improved from 3.93023 to 3.92605, saving model to Best_weights.hdf5\nEpoch 27/100\n92/92 [==============================] - 5s 58ms/step - loss: 3.9325 - val_loss: 7.9628\n\nEpoch 00027: loss did not improve from 3.92605\nEpoch 28/100\n92/92 [==============================] - 5s 56ms/step - loss: 3.9167 - val_loss: 7.9948\n\nEpoch 00028: loss improved from 3.92605 to 3.91670, saving model to Best_weights.hdf5\nEpoch 29/100\n92/92 [==============================] - ETA: 0s - loss: 3.9125","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train[10]\n_, counts = np.unique(Y_train[10], return_counts=True)\nprint(counts)\n#Y_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parametr tunning with tensorboard\n\n#PARAMETERS\nEPOCHS = 5\nHP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([256, 512]))\nHP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2]))\nHP_BATCH = hp.HParam('batch', hp.Discrete([32, 64]))\nHP_TIME = hp.HParam('time')\nHP_SEQUENCE = hp.HParam('Len_sequence')\n\n#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n\n#Directory to save logs\nlog_dir = \"./logs/\"\n\nwith tf.summary.create_file_writer(log_dir).as_default():\n    hp.hparams_config(\n        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_BATCH, HP_TIME, HP_SEQUENCE],\n        metrics=[\n        hp.Metric(tag=\"categorical_crossentropy\", display_name=\"Loss\")\n        ]\n    )\n        \n    \n#defining model with changing parameters    \ndef train_test_model(hparams, directory):\n    model = Sequential()\n    model.add(LSTM(hparams[HP_NUM_UNITS], input_shape=(X_data.shape[1], X_data.shape[2]), return_sequences=True))\n    model.add(Dropout(hparams[HP_DROPOUT]))\n    model.add(LSTM(hparams[HP_NUM_UNITS]))\n    model.add(Dropout(hparams[HP_DROPOUT]))\n    model.add(Dense(y_data.shape[1], activation=\"softmax\"))\n    model.compile(\n    optimizer=\"adam\",\n    loss='categorical_crossentropy'\n    )\n    sequence = data_sequence(X_data, y_data, hparams[HP_BATCH])\n    start_time = time.time()\n    history = model.fit(sequence, epochs=EPOCHS, callbacks=[tf.keras.callbacks.TensorBoard(f\"{directory}:{batch_size}\", histogram_freq=1)])\n    duration = time.time() - start_time\n    return history, duration\n\ndef run_experiment(directory, hparams):\n    with tf.summary.create_file_writer(directory).as_default():\n        hist, time = train_test_model(hparams, directory)\n        hparams[\"time\"] = (time)/EPOCHS\n        hparams[\"Len_sequence\"] = LEN_SEQUENCE\n        hp.hparams(hparams)\n        for step, loss in enumerate(hist.history[\"loss\"]):\n                tf.summary.scalar(\"categorical_crossentropy\", loss, step=step)\n\nsession_num = 0\n\n#training loop\nfor num_units in HP_NUM_UNITS.domain.values:\n    for dropout_rate in HP_DROPOUT.domain.values:\n        for batch_size in HP_BATCH.domain.values:\n            hparams = {\n                HP_NUM_UNITS: num_units,\n                HP_DROPOUT: dropout_rate,\n                HP_BATCH: batch_size\n                }\n            run_name = \"/run-%d\" % session_num\n            print('--- Starting trial: %s' % run_name)\n            print({h.name: hparams[h] for h in hparams})\n            run_experiment(log_dir+run_name, hparams)\n            \n            session_num += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compress folder to zip file\nimport shutil\nshutil.make_archive(\"tensorboard\", 'zip', \"./logs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GENERATE TEXT\n\n#Load model\n#SAVED_MODEL_PATH = '../input/harry-potter-gru-text-generator/Best_weights.hdf5'\n#model = keras.models.load_model(SAVED_MODEL_PATH)\n#model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Function which make our output more random\ndef temp_index(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds[0], 1)\n    return np.argmax(probas)\n\nstart = np.random.randint(0, len(X)-1)\nstart = 70\npattern = X[start]\npred_text = []\nprint(\"Seed:\")\nprint(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\nprint(\"\")\n#LOWER VALUE-MORE GENERIC/REPETITIVE/PREDICTIVE TEXT : HIGHER VALUE-MORE SUPRISING/UNPREDICTABLE TEXT\ntemperature = 1\nfor i in range(120):\n    x = np.reshape(pattern, (1, len(pattern), 1))\n    x = x / float(len(ALL_WORDS))\n    prediction = model.predict(x, verbose=0)\n    index = temp_index(prediction, temperature)\n    result = int_to_word[index]\n    seq_in = [int_to_word[value] for value in pattern]\n    pred_text.append(result)\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\nprint(' '.join(pred_text))    \nprint(\"\\nDone.\")","metadata":{"execution":{"iopub.status.busy":"2022-09-07T19:31:43.247244Z","iopub.execute_input":"2022-09-07T19:31:43.248182Z","iopub.status.idle":"2022-09-07T19:31:51.845821Z","shell.execute_reply.started":"2022-09-07T19:31:43.248133Z","shell.execute_reply":"2022-09-07T19:31:51.844868Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Seed:\n\" around the corner night \"\n\nknew knew knew centaurs bit unusual centaurs coming unusual knew quick knew knew knew bit notice knew centaurs knew bit knew knew bit pockets knew better truth crabbe leaky about knew knew teach anyway breakfast knew knew unusual supposed knew knew knew knew knew coming knew isnt knew crabbe knew useful whos air waving knew knew knew knew knew cause knew draco quick smell knew knew centaurs knew warning unusual c knew bit knew knew centaurs send knew knew knew unusual minute library knew knew knew knew knew anyway hold knew cause half knew bit wonder knew unusual his knew knew knew centaurs knew knew knew knew bit knew knew bit centaurs coming knew ourselves quick bear unusual knew knew\n\nDone.\n","output_type":"stream"}]},{"cell_type":"code","source":"SAVED_MODEL_PATH = './Best_weights.hdf5'\nmodel = keras.models.load_model(SAVED_MODEL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change LR\nfrom keras import backend as K\nprint(model.optimizer.lr)\nK.set_value(model.optimizer.learning_rate, 0.0001)\nmodel.optimizer.lr","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}