{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTS\nimport psutil\nimport re\nimport os\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.keras.utils import Sequence\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nfrom keras import backend as K\nimport math\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DIRECTORIES\nDATASET = \"../input/harry-potter-gru-text-generator\"\nDATA_PATH = \"../input/harry-potter-philosophers-stone-preprocessed/Harry_Potter_philosophers_stone.txt\"\nSAVED_MODEL_PATH = \"../input/harry-potter-gru-text-generator/Best_weights.hdf5\"\nCHECKPOINT_PATH = \"Best_weights.hdf5\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copy file from Input to Output(to easier create a new dataset with updated weights)\nfor file in os.listdir(DATASET):\n    if file.endswith('hdf5') == False:\n        path = os.path.join(DATASET, file)\n        !cp -r $path ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the file\ntext = open(DATA_PATH, \"r\", encoding=\"utf-8\").read().lower()\nwords = text.split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trying to remove sentences with low usage words. OPTIONAL\nwords_unique = Counter(words).most_common()\ndictionary = {}\nfor word in words_unique:\n    dictionary[word[0]] = word[1]\ndict_values = list(dictionary.values())\n\nwords_remove = [list(dictionary.keys())[idx] for idx, val in enumerate(dict_values) if val <= 3 and len(list(dictionary.keys())[idx]) >= 3]\nprint(len(words_remove))\nsentences = re.split('[.!?]', text)\nsent = []\nsent = [re.sub('[\\n]', '', sentence) for sentence in sentences]\nsentences = [sentence for sentence in sent if not any(word in sentence for word in words_remove)]\nprint(len(sentences))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text preprocessing\nendings  = ('.', '!', '?')\n\nfor idx, word in enumerate(words):\n    if word.endswith(endings) and word not in endings: #spliting special characters . ! ? on the end of the sentence\n        words[idx] = re.sub('[.!?]', '', word)\n        words.insert(idx+1, word[-1])\n    if words[idx].startswith('.') and word not in endings: #spliting special characters . ! ? on the beggining of the sentence\n        words[idx] = re.sub('[.]', '', word)\n        words.insert(idx-1, '.')\n    if re.search('.[.].', words[idx]): #spliting words which have . in between\n        w = word.split('.')\n        words[idx] = '.'\n        words.insert(idx-1, w[0])\n        words.insert(idx+1, w[-1])\nsentences = re.split('[.!?]', text)\nsent = []\nsent = [re.sub('[\\n]', '', sentence) for sentence in sentences]\nnew_text = ''.join(sent)\nnew_text = re.sub('  ', ' ', new_text)\nwords = new_text.split()        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting number of unique words\nprint(len(words_unique))\nplt.figure(figsize=(10,10))\nplt.plot(dict_values)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DATA PREPERATION\nLEN_SEQUENCE = 5\nWORDS_IN_TEXT = len(words)\nALL_WORDS = sorted(set(words))\n\n#Creating generator\nclass data_sequence(Sequence):\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n\n        return batch_x, batch_y\n\nword_to_int = {word:i for i, word in enumerate(ALL_WORDS)}\nint_to_word = {v: k for k, v in word_to_int.items()}\n\nX = []\ny = []\n\nperc = [per for per in range(10, 101, 10)]\nj = 0\n\nfor i in range(0, WORDS_IN_TEXT-LEN_SEQUENCE, 1):\n    sequence_X = words[i:i+LEN_SEQUENCE]\n    sequence_y = words[i+LEN_SEQUENCE]\n    X.append([word_to_int[c] for c in sequence_X])\n    y.append(word_to_int[sequence_y])   \n\npatterns = len(X)\nX_data = np.reshape(X, (patterns, LEN_SEQUENCE, 1))\ny_data = np_utils.to_categorical(y)\n\n#Data split\nval_samples = int(patterns * 0.80)\nX_train = X_data[:val_samples, :, :]\nX_test = X_data[val_samples:, :, :]\nY_train = y_data[:val_samples]\nY_test = y_data[val_samples:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRAINING PIPELINE (from weights)\n\n#PARAMETERS\nUNITS = 512\nDROPOUT = 0.2\nBATCH = 64\nEPOCH = 1\n\n#CHECKPOINT INITIALIZATION\ncheckpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\n#STOP TRAINING IF NO IMPROVE\n#early_stop = tf.keras.callbacks.EarlyStopping(\"loss\", patience=2, restore_best_weights=True)\n\n#LOAD WEIGHTS(from dataset)\nsequence = data_sequence(X_data, y_data, BATCH)\nmodel = keras.models.load_model(SAVED_MODEL_PATH)\ntensorboard_log = tf.keras.callbacks.TensorBoard(f\"./logs/{datetime.datetime.now().strftime('%d-%m-%y-%H:%M')}-L:{LEN_SEQUENCE}_U:{UNITS}_D:{DROPOUT}_B:{BATCH}\", histogram_freq=1)\n\n#CHANGE lr\nK.set_value(model.optimizer.learning_rate, 0.0001)\n\n# START TREINING\nmodel.fit(sequence, epochs=EPOCH, callbacks=[checkpoint, tensorboard_log])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRAIN FROM SCRATCH\n\n#PARAMETERS\nUNITS = 512\nDROPOUT = 0.5\nBATCH = 64\nEPOCH = 100\n\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(GRU(UNITS, input_shape=(X_data.shape[1], X_data.shape[2]), return_sequences=True))\nmodel.add(Dropout(DROPOUT))\nmodel.add(GRU(UNITS))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(y_data.shape[1], activation=\"softmax\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n\n#CHECKPOINT INITIALIZATION\ncheckpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ntrain_sequence = data_sequence(X_train, Y_train, BATCH)\ntest_sequence = data_sequence(X_test, Y_test, BATCH)\ntensorboard_log = tf.keras.callbacks.TensorBoard(f\"./logs/{datetime.datetime.now().strftime('%d-%m-%y-%H:%M')}-L:{LEN_SEQUENCE}_U:{UNITS}_D:{DROPOUT}_B:{BATCH}\", histogram_freq=1)\nmodel.fit(train_sequence, epochs=EPOCH, callbacks=[checkpoint, tensorboard_log], validation_data=test_sequence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parametr tunning with tensorboard\n\n#PARAMETERS\nEPOCHS = 5\nHP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([256, 512]))\nHP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2]))\nHP_BATCH = hp.HParam('batch', hp.Discrete([32, 64]))\nHP_TIME = hp.HParam('time')\nHP_SEQUENCE = hp.HParam('Len_sequence')\n\n#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n\n#Directory to save logs\nlog_dir = \"./logs/\"\n\nwith tf.summary.create_file_writer(log_dir).as_default():\n    hp.hparams_config(\n        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_BATCH, HP_TIME, HP_SEQUENCE],\n        metrics=[\n        hp.Metric(tag=\"categorical_crossentropy\", display_name=\"Loss\")\n        ]\n    )\n        \n    \n#defining model with changing parameters    \ndef train_test_model(hparams, directory):\n    model = Sequential()\n    model.add(LSTM(hparams[HP_NUM_UNITS], input_shape=(X_data.shape[1], X_data.shape[2]), return_sequences=True))\n    model.add(Dropout(hparams[HP_DROPOUT]))\n    model.add(LSTM(hparams[HP_NUM_UNITS]))\n    model.add(Dropout(hparams[HP_DROPOUT]))\n    model.add(Dense(y_data.shape[1], activation=\"softmax\"))\n    model.compile(\n    optimizer=\"adam\",\n    loss='categorical_crossentropy'\n    )\n    sequence = data_sequence(X_data, y_data, hparams[HP_BATCH])\n    start_time = time.time()\n    history = model.fit(sequence, epochs=EPOCHS, callbacks=[tf.keras.callbacks.TensorBoard(f\"{directory}:{batch_size}\", histogram_freq=1)])\n    duration = time.time() - start_time\n    return history, duration\n\ndef run_experiment(directory, hparams):\n    with tf.summary.create_file_writer(directory).as_default():\n        hist, time = train_test_model(hparams, directory)\n        hparams[\"time\"] = (time)/EPOCHS\n        hparams[\"Len_sequence\"] = LEN_SEQUENCE\n        hp.hparams(hparams)\n        for step, loss in enumerate(hist.history[\"loss\"]):\n                tf.summary.scalar(\"categorical_crossentropy\", loss, step=step)\n\nsession_num = 0\n\n#training loop\nfor num_units in HP_NUM_UNITS.domain.values:\n    for dropout_rate in HP_DROPOUT.domain.values:\n        for batch_size in HP_BATCH.domain.values:\n            hparams = {\n                HP_NUM_UNITS: num_units,\n                HP_DROPOUT: dropout_rate,\n                HP_BATCH: batch_size\n                }\n            run_name = \"/run-%d\" % session_num\n            print('--- Starting trial: %s' % run_name)\n            print({h.name: hparams[h] for h in hparams})\n            run_experiment(log_dir+run_name, hparams)\n            \n            session_num += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compress folder to zip file\nimport shutil\nshutil.make_archive(\"tensorboard\", 'zip', \"./logs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GENERATE TEXT\n\n#Load model\n#SAVED_MODEL_PATH = '../input/harry-potter-gru-text-generator/Best_weights.hdf5'\n#model = keras.models.load_model(SAVED_MODEL_PATH)\n#model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Function which make our output more random\ndef temp_index(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds[0], 1)\n    return np.argmax(probas)\n\nstart = np.random.randint(0, len(X)-1)\nstart = 70\npattern = X[start]\npred_text = []\nprint(\"Seed:\")\nprint(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\nprint(\"\")\ntemperature = 1 #LOWER VALUE-MORE GENERIC/REPETITIVE/PREDICTIVE TEXT : HIGHER VALUE-MORE SUPRISING/UNPREDICTABLE TEXT\nfor i in range(120):\n    x = np.reshape(pattern, (1, len(pattern), 1))\n    x = x / float(len(ALL_WORDS))\n    prediction = model.predict(x, verbose=0)\n    index = temp_index(prediction, temperature)\n    result = int_to_word[index]\n    seq_in = [int_to_word[value] for value in pattern]\n    pred_text.append(result)\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\nprint(' '.join(pred_text))    \nprint(\"\\nDone.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change LR\nfrom keras import backend as K\nprint(model.optimizer.lr)\nK.set_value(model.optimizer.learning_rate, 0.0001)\nmodel.optimizer.lr","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}