{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warhammer RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.1.1\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp311-cp311-linux_x86_64.whl size=4237780 sha256=a44c73baeae24e8bd4bd0a04000f45f4ad24696308700717125364085b958d3b\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/b6/cf/7315ec7b0149210d2d4447d9c3338b36d10e56a1ecddcd35c0\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llama-cpp-python][llama-cpp-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.14\n",
      "Collecting cerebras-cloud-sdk\n",
      "  Downloading cerebras_cloud_sdk-1.35.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->cerebras-cloud-sdk) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras-cloud-sdk) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.4.0)\n",
      "Downloading cerebras_cloud_sdk-1.35.0-py3-none-any.whl (89 kB)\n",
      "Installing collected packages: cerebras-cloud-sdk\n",
      "Successfully installed cerebras-cloud-sdk-1.35.0\n",
      "Collecting pl-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.8.0/pl_core_news_lg-3.8.0-py3-none-any.whl (573.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.7/573.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-lg\n",
      "Successfully installed pl-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.49->langchain)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Collecting langchain-core<1.0.0,>=0.3.49 (from langchain)\n",
      "  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting langsmith>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.8-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (25.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.0rc1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=7a30492e88ac1fbb68d3c3399dc42178d0a7bf86e6be055e6f0dd3b2d77ada22\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, pybase64, protobuf, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mmh3, importlib-metadata, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, langsmith, kubernetes, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain, onnxruntime, langchain-community, chromadb\n",
      "\u001b[2K  Attempting uninstall: protobuf━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: protobuf 3.20.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling protobuf-3.20.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled protobuf-3.20.3━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: packaging━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/42\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.9.41━━━━\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.9.41:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41━━━━━━\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.10.19\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.10.19:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.10.19━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.4.0.6━━━\u001b[0m \u001b[32m10/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.4.0.6:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6━━━━━━━━━\u001b[0m \u001b[32m11/42\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/42\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.9.0.13━\u001b[0m \u001b[32m11/42\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.9.0.13:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/42\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13━━━━━━━\u001b[0m \u001b[32m12/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: importlib-metadata━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0━━━━━\u001b[0m \u001b[32m12/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0━━━━━━━━━━━\u001b[0m \u001b[32m14/42\u001b[0m [importlib-metadata]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-apim\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [posthog]s]data]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.31.1━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [posthog]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.31.1:m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [posthog]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.31.1━━━━━━━━━━━\u001b[0m \u001b[32m23/42\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/42\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\u001b[0m \u001b[32m23/42\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.9.5:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/42\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m24/42\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75━━━━━━━━━\u001b[0m \u001b[32m25/42\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions━━━━━━━━\u001b[0m \u001b[32m28/42\u001b[0m [pydantic-settings]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.52b1/42\u001b[0m [pydantic-settings]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.52b1:━━━\u001b[0m \u001b[32m28/42\u001b[0m [pydantic-settings]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1[32m29/42\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m30/42\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.4.40[0m \u001b[32m30/42\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.4.40:90m━━━━━━━━━━━\u001b[0m \u001b[32m30/42\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40━━━━━\u001b[0m \u001b[32m31/42\u001b[0m [nvidia-cusolver-cu12]lp-proto-common]\n",
      "\u001b[2K  Attempting uninstall: langsmith━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m31/42\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: langsmith 0.3.2390m━━━━━━━━━━\u001b[0m \u001b[32m31/42\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling langsmith-0.3.23:[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m31/42\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.3.23╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m32/42\u001b[0m [langsmith]r-cu12]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdk0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m33/42\u001b[0m [kubernetes]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.31.1━━━━━\u001b[0m \u001b[32m33/42\u001b[0m [kubernetes]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.31.1:0m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m33/42\u001b[0m [kubernetes]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.31.1━━━━━━━\u001b[0m \u001b[32m33/42\u001b[0m [kubernetes]\n",
      "\u001b[2K  Attempting uninstall: langchain-core━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m34/42\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.50m━━━━━━━\u001b[0m \u001b[32m34/42\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.50:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m34/42\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.50[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain-text-splitters0m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-text-splitters 0.3.7[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-text-splitters-0.3.7:[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-text-splitters-0.3.7━\u001b[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m35/42\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain 0.3.22[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m38/42\u001b[0m [langchain]]\n",
      "\u001b[2K    Uninstalling langchain-0.3.22:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m38/42\u001b[0m [langchain]\n",
      "\u001b[2K      Successfully uninstalled langchain-0.3.22m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m38/42\u001b[0m [langchain]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [chromadb]chromadb]langchain-community]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "wandb 0.19.9 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 6.31.1 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-aiplatform 1.87.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.72 langchain-text-splitters-0.3.9 langsmith-0.4.8 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 packaging-24.2 posthog-5.4.0 protobuf-6.31.1 pybase64-1.4.2 pydantic-settings-2.10.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install llama-cpp-python\n",
    "! pip install cerebras-cloud-sdk\n",
    "!python -m spacy download pl_core_news_lg\n",
    "!pip install langchain langchain-community sentence-transformers chromadb\n",
    "!pip install pypdf requests pydantic tqdm\n",
    "!pip install rank_bm25\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import spacy\n",
    "import torch\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n",
    "from chromadb.config import Settings\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from spacy.tokens import Doc\n",
    "from typing import List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load secret keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "cerebras_key = user_secrets.get_secret(\"Cerebras_ai_api\")\n",
    "openrouter_key = user_secrets.get_secret(\"OPENROUTER_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our document\n",
    "def load_document(doc_path: str):\n",
    "    loader = PyPDFLoader(data_path)\n",
    "    wh_rulebook = loader.load()\n",
    "    wh_rulebook_content = [content.page_content for content in wh_rulebook]\n",
    "    #wh_rulebook_raw = ''\n",
    "    #wh_rulebook_raw = ' '.join(wh_rulebook_content)\n",
    "    return wh_rulebook_content\n",
    "\n",
    "# split our data into chunks (one chunk - one page)\n",
    "def smart_overlap_pages(pages, overlap_chars: int = 200):\n",
    "    overlapped_chunks = []\n",
    "    for i in range(len(pages)):\n",
    "        prev = pages[i - 1] if i > 0 else \"\"\n",
    "        next = pages[i + 1] if i < len(pages) - 1 else \"\"\n",
    "\n",
    "        prev_overlap = prev[-overlap_chars:] if len(prev) > overlap_chars else prev\n",
    "        next_overlap = next[:overlap_chars] if len(next) > overlap_chars else next\n",
    "\n",
    "        chunk = prev_overlap + \"\\n\" + pages[i] + \"\\n\" + next_overlap\n",
    "        overlapped_chunks.append(chunk)\n",
    "    return overlapped_chunks\n",
    "\n",
    "# initialize our embeddings model\n",
    "def initialize_model(model_name: str):\n",
    "    # encoder - initialize our encoder to create embedings\n",
    "    # sdadas/mmlw-roberta-base - 124m par\n",
    "    # sdadas/mmlw-retrieval-roberta-large-v2 435m par\n",
    "\n",
    "    model = SentenceTransformer(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device=None,\n",
    "        model_kwargs={\"trust_remote_code\": True}\n",
    "        #model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"trust_remote_code\": True} Not implemented yet for roberta base\n",
    "    )\n",
    "    model.bfloat16\n",
    "    return model\n",
    "\n",
    "# create embedings for our document\n",
    "def create_embedings(model, wh_content_chunked):\n",
    "    query_prefix = \"zapytanie: \"\n",
    "    answer_prefix = \"\"\n",
    "    wh_embedings = []\n",
    "\n",
    "    for content in wh_content_chunked:\n",
    "        queries = [query_prefix + content]\n",
    "        encode = model.encode(queries, show_progress_bar=False)\n",
    "        wh_embedings.append(encode)\n",
    "\n",
    "    wh_embedings = torch.tensor(wh_embedings, dtype=torch.bfloat16)\n",
    "    wh_embedings = wh_embedings.squeeze(1)\n",
    "    return wh_embedings\n",
    "\n",
    "# split our text into max size chunks\n",
    "def chunk_text(text, chunk_size=100000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"Wprowadzenie\"\n",
    "start = re.finditer(pattern, wh_rulebook_raw)\n",
    "\n",
    "wh_rulebook_raw[start-10:start+1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"warhammer fantasy\", persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Initialize ChromaDB vector store\"\"\"\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # Create or get collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, chunks, embeddings):\n",
    "        \"\"\"Add document chunks to vector store\"\"\"\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "        \n",
    "        self.collection.add(\n",
    "            documents=[chunk for chunk in chunks],\n",
    "            embeddings=embeddings.tolist(),\n",
    "            ids=ids\n",
    "        )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_dict=None):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=k,\n",
    "            where=filter_dict\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Additional **optional** steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid retriever\n",
    "\n",
    "Combine cos similarity method with additional one which check words frequency in our query(BM25) to choose best available chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - additional retrievel to improve our performance\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, embedding_model):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_model = embedding_model\n",
    "        self.bm25 = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def setup_bm25(self, documents):\n",
    "        \"\"\"Setup BM25 for keyword-based retrieval\"\"\"\n",
    "        self.documents = documents\n",
    "        tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Simple tokenization for BM25\"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    def retrieve(self, query, k=10, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining semantic and keyword search\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            k: Number of documents to retrieve\n",
    "            alpha: Weight for semantic search (1-alpha for BM25)\n",
    "        \"\"\"\n",
    "        # Semantic search\n",
    "        query = \"Zapytanie: {query}\"\n",
    "        query_embedding = self.embedding_model.encode(query, show_progress_bar=False)\n",
    "        semantic_results = self.vector_store.similarity_search(query_embedding, k=k*2)\n",
    "        \n",
    "        # BM25 search\n",
    "        if self.bm25:\n",
    "            tokenized_query = self._tokenize(query)\n",
    "            bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "            bm25_results = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:k*2]\n",
    "        else:\n",
    "            bm25_results = []\n",
    "        \n",
    "        # Combine and rerank results\n",
    "        combined_results = self._combine_results(semantic_results, bm25_results, alpha)\n",
    "        \n",
    "        return combined_results[:k]\n",
    "    \n",
    "    def _combine_results(self, semantic_results, bm25_results, alpha):\n",
    "        \"\"\"\n",
    "        Combine semantic and BM25 results with weighted scoring using document index.\n",
    "\n",
    "        Args:\n",
    "            semantic_results: List of document objects from vector store\n",
    "            bm25_results: List of (index, score) tuples from BM25\n",
    "            alpha: Weight for semantic scores\n",
    "\n",
    "        Returns:\n",
    "            List of combined documents reranked by hybrid score\n",
    "        \"\"\"\n",
    "        combined_scores = defaultdict(float)\n",
    "\n",
    "        # Normalize BM25 scores\n",
    "        if bm25_results:\n",
    "            bm25_values = np.array([score for _, score in bm25_results])\n",
    "            bm25_norm = (bm25_values - bm25_values.min()) / (bm25_values.ptp() + 1e-9)\n",
    "            for (idx, _), score in zip(bm25_results, bm25_norm):\n",
    "                combined_scores[idx] += (1 - alpha) * score\n",
    "\n",
    "        # Normalize semantic scores\n",
    "        semantic_values = np.array([doc.score if hasattr(doc, \"score\") else 1.0 for doc in semantic_results])\n",
    "        semantic_norm = (semantic_values - semantic_values.min()) / (semantic_values.ptp() + 1e-9)\n",
    "        for idx, (doc, score) in enumerate(zip(semantic_results, semantic_norm)):\n",
    "            combined_scores[idx] += alpha * score\n",
    "\n",
    "        # Rerank based on combined scores\n",
    "        ranked_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return document objects using indices from semantic_results\n",
    "        combined_docs = [semantic_results[idx] for idx, _ in ranked_indices if idx < len(semantic_results)]\n",
    "        \n",
    "        return combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -2398.235319707179),\n",
       " (353, -3651.9779336074557),\n",
       " (1, -4414.512119553013),\n",
       " (265, -4445.269298931796),\n",
       " (345, -4662.766154168548),\n",
       " (352, -4734.125405567533),\n",
       " (348, -4734.689860508153),\n",
       " (350, -4738.656630040257),\n",
       " (347, -4739.911627819456),\n",
       " (351, -4740.795987552746),\n",
       " (17, -4742.711883337711),\n",
       " (2, -4744.965502502055),\n",
       " (349, -4748.159446805872),\n",
       " (19, -4754.374965831195),\n",
       " (18, -4756.551181832996),\n",
       " (346, -4757.109619757759),\n",
       " (16, -4757.811666166828),\n",
       " (9, -4761.3473056319735),\n",
       " (13, -4769.642190235066),\n",
       " (3, -4772.364365148799)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm = BM25Okapi(wh_rulebook_overlap)\n",
    "test = wh_rulebook_overlap[25]\n",
    "bm25_scores = bm.get_scores(test)\n",
    "bm25_results = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:10*2]\n",
    "bm25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Cerebras(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=cerebras_key\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"What is the meaning of life?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b\",\n",
    "    stream=True,\n",
    "    max_completion_tokens=10240,\n",
    "    temperature=0.7,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Tests\n",
    "# gemini - 6.5\n",
    "# kimi - 5.4\n",
    "# R1 - 21.8\n",
    "# V3 - 12.7\n",
    "# chimera - 35.6\n",
    "# qwen - 29.2\n",
    "\n",
    "models = {'r1':'deepseek/deepseek-r1-0528:free',\n",
    "         'v3':'deepseek/deepseek-chat-v3-0324:free',\n",
    "         'chimera':'tngtech/deepseek-r1t2-chimera:free',\n",
    "         'kimi':'moonshotai/kimi-k2:free',\n",
    "         'gemini':'google/gemini-2.0-flash-exp:free',\n",
    "         'qwen': 'qwen/qwq-32b:free'\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=openrouter_key,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
    "  },\n",
    "  extra_body={},\n",
    "  model = models['qwen'],\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the meaning of life?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for entities in our doc\n",
    "nlp = spacy.load(\"pl_core_news_lg\")  # or \"md\", \"lg\" for larger models\n",
    "\n",
    "chunks = chunk_text(wh_rulebook_raw)\n",
    "docs = [nlp(chunk) for chunk in chunks]\n",
    "combined_doc = Doc.from_docs(docs)\n",
    "entities = [(ent.text, ent.label_) for ent in combined_doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query enchancer\n",
    "\n",
    "Improve our question by generating additional querys which will help model to answer our question.  \n",
    "Additionaly it check if some popular entities are not present in our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEnhancer:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Cerebras(\n",
    "        api_key=api_key\n",
    "        )\n",
    "    \n",
    "    def expand_warhammer_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate multiple query variations for better retrieval\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Biorąc pod uwagę zapytanie dotyczące Warhammer Fantasy: '{query}'\n",
    "        \n",
    "        Wygeneruj 3 alternatywne sformułowania, które mogą pomóc w znalezieniu odpowiednich informacji:\n",
    "        1. Bardziej szczegółowa wersja z terminologią Warhammer Fantasy\n",
    "        2. Szersza wersja obejmująca powiązane pojęcia\n",
    "        3. Wersja skupiająca się na zasadach/mechanice, jeśli ma to zastosowanie\n",
    "        \n",
    "        Zwróć tylko 3 zapytania, po jednym w każdym wierszu.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "        extra_body={},\n",
    "        model = 'llama-3.3-70b',\n",
    "        max_completion_tokens=10240,\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        variations = response.choices[0].message.content.strip().split('\\n')\n",
    "        return [query] + [v.strip() for v in variations if v.strip()]\n",
    "    \n",
    "    def extract_entities(self, query: str) -> dict:\n",
    "        \"\"\"Extract Warhammer 40k entities from query\"\"\"\n",
    "        warhammer_entities = {\n",
    "            'factions': ['space marines', 'orks', 'eldar', 'tau', 'necrons', 'chaos'],\n",
    "            'unit_types': ['infantry', 'vehicle', 'monster', 'character'],\n",
    "            'weapons': ['bolter', 'lasgun', 'plasma', 'melta'],\n",
    "            'rules': ['armor save', 'weapon skill', 'ballistic skill']\n",
    "        }\n",
    "        \n",
    "        found_entities = {}\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for category, entities in warhammer_entities.items():\n",
    "            found = [entity for entity in entities if entity in query_lower]\n",
    "            if found:\n",
    "                found_entities[category] = found\n",
    "        \n",
    "        return found_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRanker:\n",
    "    def __init__(self, model_name=\"radlab/polish-cross-encoder\"):\n",
    "        \"\"\"Initialize cross-encoder for reranking\"\"\"\n",
    "        self.reranker = CrossEncoder(model_name)\n",
    "    \n",
    "    def rerank_contexts(self, query: str, contexts: List[str], top_k: int = 5) -> List[dict]:\n",
    "        \"\"\"Rerank retrieved contexts using cross-encoder\"\"\"\n",
    "        # Create query-context pairs\n",
    "        pairs = [(query, context) for context in contexts]\n",
    "        \n",
    "        # Get relevance scores\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort by score and return top_k\n",
    "        scored_contexts = [\n",
    "            {'context': context, 'score': score}\n",
    "            for context, score in zip(contexts, scores)\n",
    "        ]\n",
    "        \n",
    "        return sorted(scored_contexts, key=lambda x: x['score'], reverse=True)[:top_k]\n",
    "    \n",
    "    def filter_by_relevance(self, ranked_contexts: List[dict], threshold: float = 0.3) -> List[dict]:\n",
    "        \"\"\"Filter contexts by relevance threshold\"\"\"\n",
    "        return [ctx for ctx in ranked_contexts if ctx['score'] > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarhammerRAG:\n",
    "    def __init__(self, retriever, ranker, llm_client):\n",
    "        self.retriever = retriever\n",
    "        self.ranker = ranker\n",
    "        self.llm_client = llm_client\n",
    "    \n",
    "    def generate_response(self, query: str, max_context_length: int = 10240) -> dict:\n",
    "        \"\"\"Generate response using RAG pipeline\"\"\"\n",
    "        \n",
    "        # 1. Retrieve relevant contexts\n",
    "        retrieved_contexts = self.retriever.retrieve(query, k=10)\n",
    "        \n",
    "        # 2. Rerank contexts\n",
    "        context_texts = [ctx['document'] for ctx in retrieved_contexts]\n",
    "        ranked_contexts = self.ranker.rerank_contexts(query, context_texts, top_k=5)\n",
    "        \n",
    "        # 3. Select contexts within token limit\n",
    "        selected_contexts = self._select_contexts_by_length(ranked_contexts, max_context_length)\n",
    "        \n",
    "        # 4. Generate response\n",
    "        response = self._generate_with_context(query, selected_contexts)\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': selected_contexts,\n",
    "            'retrieved_count': len(retrieved_contexts)\n",
    "        }\n",
    "    \n",
    "    def _select_contexts_by_length(self, contexts: List[dict], max_length: int) -> List[dict]:\n",
    "        \"\"\"Select contexts that fit within token limit\"\"\"\n",
    "        selected = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for ctx in contexts:\n",
    "            ctx_length = len(ctx['context'])  # Simplified length calculation\n",
    "            if current_length + ctx_length <= max_length:\n",
    "                selected.append(ctx)\n",
    "                current_length += ctx_length\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def _generate_with_context(self, query: str, contexts: List[dict]) -> str:\n",
    "        \"\"\"Generate response using LLM with retrieved context\"\"\"\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([f\"[Source {i+1}]: {ctx['context']}\" \n",
    "                                   for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Jesteś asystentem eksperta ds. Warhammera fantasy. Wykorzystaj podany kontekst, aby dokładnie odpowiedzieć na pytanie użytkownika.\n",
    "\n",
    "        Kontekst z materiałów fantastyki Warhammer: {context_text}\n",
    "        Pytanie użytkownika: {query}\n",
    "\n",
    "        Instrukcje:\n",
    "        - Opieraj swoją odpowiedź przede wszystkim na podanym kontekście\n",
    "        - Jeśli kontekst nie zawiera wystarczających informacji, powiedz to jasno\n",
    "        - Używaj odpowiedniej terminologii Warhammer Fantasy\n",
    "        - W razie potrzeby uwzględnij odpowiednie zasady, statystyki lub wiedzę\n",
    "        - Podaj źródła, do których się odwołujesz (np. „Według źródła 1...”)\n",
    "\n",
    "        Odpowiedź:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            extra_body={},\n",
    "            model = 'llama-3.3-70b',\n",
    "            max_completion_tokens=10240,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever initialized - 5/9\n",
      "Enhanced query generated - 6/9\n",
      "Ranker initlialized - 7/9\n",
      "LLM Loaded - 8/9\n",
      "RAG ready - 9/9\n"
     ]
    }
   ],
   "source": [
    "# define our querry\n",
    "query = 'Podaj mi kilka interesujących pomysłów na stworzenie postaci uczonego studenta do sesji RPG w świecie Warhammer Fantasy.'\n",
    "query = 'Czy w grze RPG warhammer fantasy 4e mogę korzystać z dwóch kusz jednocześnie?'\n",
    "\n",
    "'''\n",
    "# load document\n",
    "doc_path = '/kaggle/input/warhammer-4e-rpg/WFRP_4_ed_PL_1.3.pdf'\n",
    "wh_content = load_document(doc_path)\n",
    "print(\"Document loaded - 1/9\")\n",
    "\n",
    "# split our document into chunks\n",
    "wh_content_chunked = smart_overlap_pages(wh_content)\n",
    "print(\"Document chunked - 2/9\")\n",
    "\n",
    "# initialize our embedding model and create emebeddings for our document\n",
    "model = initialize_model(\"sdadas/mmlw-roberta-base\")\n",
    "wh_embedings = create_embedings(model, wh_content_chunked)\n",
    "print(\"Embeddings prepared - 3/9\")\n",
    "\n",
    "# create our vector database and add our document into it\n",
    "vector_store = VectorStore(\"warhammer_fantasy\")\n",
    "vector_store.add_documents(wh_content_chunked, wh_embedings)\n",
    "print(\"Vector store ready - 4/9\")\n",
    "'''\n",
    "# Initialize our hybrid retriever for better content picking\n",
    "retriever = HybridRetriever(vector_store, model)\n",
    "retriever.setup_bm25(wh_rulebook_overlap)\n",
    "print(\"Retriever initialized - 5/9\")\n",
    "\n",
    "# generate additional context for our RAG\n",
    "Enhancer = QueryEnhancer(api_key=cerebras_key)\n",
    "query_enhanced = Enhancer.expand_warhammer_query(query)\n",
    "print(\"Enhanced query generated - 6/9\")\n",
    "\n",
    "# 6. Setup reranker\n",
    "ranker = ContextRanker()\n",
    "print(\"Ranker initlialized - 7/9\")\n",
    "\n",
    "# 7. Setup LLM client\n",
    "llm_client = Cerebras(api_key=cerebras_key)\n",
    "print(\"LLM Loaded - 8/9\")\n",
    "\n",
    "# 8. Create RAG system\n",
    "rag_system = WarhammerRAG(retriever, ranker, llm_client)\n",
    "print(\"RAG ready - 9/9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Czy w grze RPG warhammer fantasy 4e mogę korzystać z dwóch kusz jednocześnie?',\n",
       " 'Czy w Warhammer Fantasy 4e można używać dwóch kuszy jako podwójne bronie dystansowe?',\n",
       " 'Czy istnieją ograniczenia dotyczące korzystania z wielu broni palnych w grach RPG?',\n",
       " 'Jaki jest mechanizm obsługi broni podwójnych w Warhammer Fantasy 4e w kontekście kuszy?']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/131185114.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrag_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWarhammerRAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_enhanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/3805431562.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(self, query, max_context_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# 1. Retrieve relevant contexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mretrieved_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 2. Rerank contexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/415731426.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, query, k, alpha)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Combine and rerank results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mcombined_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/415731426.py\u001b[0m in \u001b[0;36m_combine_results\u001b[0;34m(self, semantic_results, bm25_results, alpha)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Return document objects using indices from semantic_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcombined_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msemantic_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/415731426.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Return document objects using indices from semantic_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcombined_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msemantic_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "rag_system = WarhammerRAG(retriever, ranker, llm_client)\n",
    "response = rag_system.generate_response(query_enhanced)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into chunks\n",
    "\n",
    "separators = [\n",
    "r\"\\n.*?•\\n\", # New chapter\n",
    "r\"\\n.*?\\n\",  # New topic\n",
    "r\"\\n\\n\",     # Paragraphs\n",
    "r\"\\n\",       # Lines\n",
    "r\".\",        # Sentences\n",
    "]\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = model\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    #separators=separators,\n",
    "    length_function=lambda x: len(encoding.encode(\"zapytanie: {x}\", convert_to_tensor=True, show_progress_bar=False)),\n",
    "    #is_separator_regex=True,\n",
    "    #keep_separator=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(wh_rulebook_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('page_content', 'WARHAMMER FANTASY ROLEPLAY\\n6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nRandom content\\n', '\\nMore content\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\n.*?\\n\"\n",
    "text = \"Some text\\nRandom content\\nAnother section\\nMore content\\nFinal part.\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  # Check if it correctly identifies separators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# check optimal chunk size\n",
    "# check optimal chunk overlap\n",
    "# check best separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('page_content', '•\\nA zatem, czego tu szukasz? Przygody?\\nByć może. Złota? Zapewne. Sprawiedliwości?\\nHa, to dość względne pojęcie! Cóż to? Świętoszkowaty błysk w twym oku? A, chcesz robić \\nto, co jest właściwe... Dopóki jest to dobrze płatne, dostarcza Ci rozrywki i pasuje do Twoich \\npoglądów. Niech będzie, to wystarczy. Nadasz się. Wejdź, opowiem Ci o tej robocie.\\nGdy wymagany jest rezultat losowy, gra WFRP korzysta \\nz dziesięciościennej kostki. Dziesięciościenne kostki zazwyczaj \\nmają ścianki oznakowane od 0 do 9, gdzie rzut 0 liczy się jako \\n10. W zasadach takie kości określane są jako k10, a ich liczba, \\nktórą trzeba rzucić, zawsze jest podana w następujący sposób: \\n1k10 za jedną kostkę, 2k10 za dwie kostki, 3k10 za trzy kostki \\ni tak dalej.\\nJeśli należy rzucić kilkoma kośćmi, wyniki są zawsze \\nsumowane. Zatem jeśli zasady proszą o rzucenie 2k10, rzucasz \\ndwoma dziesięciościennymi kośćmi i dodajesz wyniki ich obu, \\nna przykład rzut 0 i 3 oznacza wynik 13 (10+3=13).\\nCzasami rzut kostką zostanie zmodyfikowany przez dodanie \\nlub odjęcie liczby. Zatem rzut 1k10+4 oznacza rzut jedną \\ndziesięciościenną kostką i dodanie do wyniku 4, natomiast rzut \\n2k10-3 wskazuje, że należy rzucić dwoma dziesięciościennymi \\nkośćmi i odjąć od sumarycznego wyniku 3.\\nPonadto zasady wykorzystują rzut dwoma dziesięciościennymi \\nkośćmi do uzyskania liczby od 1 do 100 (oznakowane jako \\n1k100). Aby to zrobić, jedna dziesięciościenna kostka zostaje \\nuznana za kość „dziesiątek”, a druga za kość „jedności”. Teraz \\nrzuć dwoma kośćmi i odczytaj wynik jako liczbę dwucyfrową. \\nUwaga, w tym przypadku wynik „0” na kostce odczytujemy \\nzawsze właśnie jako zero! Zatem rzut 1 na kostce dziesiątek i 4 \\nna kostce jedności daje wynik 14, a rzut 4 i 2 oznacza 42. Jeśli \\nna obu kościach wypadło 0, wynik wynosi 100.\\nKOŚCI ZOSTAŁY RZUCONE\\nTomasz Otto (Order #44833549)')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7420642,
     "sourceId": 11814496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
