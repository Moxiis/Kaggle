{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warhammer RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.2\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl size=4422297 sha256=4a4c0ca3c5fd0b138c98710cd1e604f214ae1b0e5da9c2ab305e48ffc0eaa7bb\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/5b/e5/a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llama-cpp-python][llama-cpp-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
      "Collecting cerebras-cloud-sdk\n",
      "  Downloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->cerebras-cloud-sdk) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras-cloud-sdk) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.4.0)\n",
      "Downloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl (91 kB)\n",
      "Installing collected packages: cerebras-cloud-sdk\n",
      "Successfully installed cerebras-cloud-sdk-1.50.1\n",
      "Collecting pl-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.8.0/pl_core_news_lg-3.8.0-py3-none-any.whl (573.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.7/573.7 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-lg\n",
      "Successfully installed pl-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.49->langchain)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Collecting langchain-core<1.0.0,>=0.3.49 (from langchain)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Collecting langsmith>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.25-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (25.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.0rc1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading langsmith-0.4.25-py3-none-any.whl (379 kB)\n",
      "Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=0f659c110171ab0e4822d3f096ea8566f20f5cc14020e69224b348424aa536a6\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, uvicorn, requests, python-dotenv, pyproject_hooks, pybase64, protobuf, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mmh3, importlib-metadata, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, langsmith, kubernetes, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain, onnxruntime, langchain-community, chromadb\n",
      "\u001b[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/43\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: requests 2.32.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/43\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling requests-2.32.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/43\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.3━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/43\u001b[0m [uvicorn]\n",
      "\u001b[2K  Attempting uninstall: protobuf━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K    Found existing installation: protobuf 3.20.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K    Uninstalling protobuf-3.20.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K      Successfully uninstalled protobuf-3.20.3━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/43\u001b[0m [protobuf]ks]\n",
      "\u001b[2K  Attempting uninstall: packagingm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/43\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.9.41:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.10.19\u001b[0m \u001b[32m10/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.10.19:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.10.19━━━━━━\u001b[0m \u001b[32m11/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.4.0.6━━━━━━━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.4.0.6:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6━━━━━━━━━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.9.0.13━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.9.0.13:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: importlib-metadata━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-apim\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [posthog]es]ly]]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.31.1━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [posthog]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.31.1:m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [posthog]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.31.1━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [posthog]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/43\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\u001b[0m \u001b[32m24/43\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.9.5:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/43\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m25/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:0m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75━━━━━━━━━\u001b[0m \u001b[32m26/43\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions━━━━━━━━\u001b[0m \u001b[32m28/43\u001b[0m [build]dlogs]-cu12]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.52b1/43\u001b[0m [build]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.52b1:━━━\u001b[0m \u001b[32m28/43\u001b[0m [build]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1[32m30/43\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.4.40[0m \u001b[32m30/43\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.4.40:0m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusolver-cu12]nventions]\n",
      "\u001b[2K  Attempting uninstall: langsmith━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: langsmith 0.3.2390m━━━━━━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling langsmith-0.3.23:[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.3.23╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m33/43\u001b[0m [langsmith]r-cu12]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdk0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [kubernetes]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.31.1━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [kubernetes]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.31.1:1m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [kubernetes]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.31.1[90m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K  Attempting uninstall: langchain-core0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.50m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.50:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.50[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain-text-splitters0m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-text-splitters 0.3.7[0m \u001b[32m36/43\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-text-splitters-0.3.7:[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-text-splitters-0.3.7━\u001b[0m \u001b[32m36/43\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [langchain-text-splitters]\n",
      "\u001b[2K    Found existing installation: langchain 0.3.22\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [langchain-text-splitters]\n",
      "\u001b[2K    Uninstalling langchain-0.3.22:━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m39/43\u001b[0m [langchain]-splitters]\n",
      "\u001b[2K      Successfully uninstalled langchain-0.3.22m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m39/43\u001b[0m [langchain]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43/43\u001b[0m [chromadb]chromadb]langchain-community]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "wandb 0.19.9 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 6.32.0 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.87.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.32.0 which is incompatible.\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.75 langchain-text-splitters-0.3.11 langsmith-0.4.25 mmh3-5.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 packaging-24.2 posthog-5.4.0 protobuf-6.32.0 pybase64-1.4.2 pydantic-settings-2.10.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 requests-2.32.5 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install llama-cpp-python\n",
    "!pip install cerebras-cloud-sdk\n",
    "!python -m spacy download pl_core_news_lg\n",
    "!pip install langchain langchain-community sentence-transformers chromadb\n",
    "!pip install pypdf requests pydantic tqdm\n",
    "!pip install rank_bm25\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 19:41:28.656309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757274088.884964      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757274088.950369      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import spacy\n",
    "import torch\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n",
    "from chromadb.config import Settings\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from spacy.tokens import Doc\n",
    "from typing import List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load secret keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "cerebras_key = user_secrets.get_secret(\"Cerebras_ai_api\")\n",
    "openrouter_key = user_secrets.get_secret(\"OPENROUTER_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document_Loader:\n",
    "    def __init__(self, \n",
    "                doc_path: str = '/kaggle/input/warhammer-4e-rpg/WFRP_4_ed_PL_1.3.pdf', \n",
    "                phrase_to_remove: str = 'Tomasz Otto (Order #44833549)',\n",
    "                start_pg: int = 0, \n",
    "                end_pg: int = -1):\n",
    "        self.doc_path = doc_path\n",
    "        self.phrase_to_remove = phrase_to_remove\n",
    "        self.start_pg = start_pg\n",
    "        self.end_pg = end_pg\n",
    "        self.wh_rulebook_content = None\n",
    "\n",
    "    def load_document(self):\n",
    "        loader = PyPDFLoader(self.doc_path)\n",
    "        wh_rulebook = loader.load()\n",
    "        wh_rulebook_content = [\n",
    "                        content.page_content.replace(self.phrase_to_remove, \"\")\n",
    "                        for content in wh_rulebook[self.start_pg:self.end_pg] # skip couple pages (title, table of contents, etc)\n",
    "    ]   \n",
    "        \n",
    "        print(\"Document loaded\")\n",
    "        self.wh_rulebook_content = wh_rulebook_content\n",
    "        return wh_rulebook_content\n",
    "    \n",
    "    def smart_overlap_pages(self, overlap_chars: int = 200):\n",
    "        overlapped_chunks = []\n",
    "        for i in range(len(self.wh_rulebook_content)):\n",
    "            prev = self.wh_rulebook_content[i - 1] if i > 0 else \"\"\n",
    "            next = self.wh_rulebook_content[i + 1] if i < len(self.wh_rulebook_content) - 1 else \"\"\n",
    "\n",
    "            prev_overlap = prev[-overlap_chars:] if len(prev) > overlap_chars else prev\n",
    "            next_overlap = next[:overlap_chars] if len(next) > overlap_chars else next\n",
    "\n",
    "            chunk = prev_overlap + \"\\n\" + self.wh_rulebook_content[i] + \"\\n\" + next_overlap\n",
    "            overlapped_chunks.append(chunk)\n",
    "\n",
    "        print(\"Document chunked\")\n",
    "        return overlapped_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeding_Creator:\n",
    "    def __init__(self, model_name: str = \"sdadas/mmlw-roberta-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.initialize_model()\n",
    "        \n",
    "    # initialize our embeddings model\n",
    "    def initialize_model(self):\n",
    "        # encoder - initialize our encoder to create embedings\n",
    "        # sdadas/mmlw-roberta-base - 124m par\n",
    "        # sdadas/mmlw-retrieval-roberta-large-v2 435m par\n",
    "\n",
    "        model = SentenceTransformer(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True,\n",
    "            device=None,\n",
    "            model_kwargs={\"trust_remote_code\": True}\n",
    "        )\n",
    "        model.bfloat16\n",
    "        \n",
    "        print(\"Embeddings model initialized\")\n",
    "        return model\n",
    "\n",
    "    # create embedings for our document\n",
    "    def create_embedings(self, wh_content_chunked):\n",
    "        query_prefix = \"zapytanie: \"\n",
    "        answer_prefix = \"\"\n",
    "        wh_embedings = []\n",
    "\n",
    "        for content in wh_content_chunked:\n",
    "            queries = [query_prefix + content]\n",
    "            encode = self.model.encode(queries, show_progress_bar=False)\n",
    "            wh_embedings.append(encode)\n",
    "\n",
    "        wh_embedings = torch.tensor(wh_embedings, dtype=torch.bfloat16)\n",
    "        wh_embedings = wh_embedings.squeeze(1)\n",
    "\n",
    "        print('Embedings ready')\n",
    "        return wh_embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide document by chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = '/kaggle/input/warhammer-4e-rpg/WFRP_4_ed_PL_1.3.pdf'\n",
    "phrase_to_remove = 'Tomasz Otto (Order #44833549)'\n",
    "\n",
    "loader = PyPDFLoader(doc_path)\n",
    "wh_rulebook = loader.load()\n",
    "wh_rulebook_content = [\n",
    "                    content.page_content.replace(phrase_to_remove, \"\")\n",
    "                    for content in wh_rulebook[6:-10] # skip table of contents and intro pages\n",
    "] \n",
    "\n",
    "#why_rulebook_raw = ' '.join(wh_rulebook_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create a histogram of word lengths\\nplt.hist(len(all_words), bins=range(1, max(len(all_words)) + 2), edgecolor='black', alpha=0.7)\\n\\n# Add labels and title\\nplt.xlabel('Word Length')\\nplt.ylabel('Frequency')\\nplt.title('Distribution of Word Lengths Between \\\\n Patterns')\\n\\n# Display the plot\\nplt.show()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "separators = []\n",
    "\n",
    "# Sample text\n",
    "for content in wh_rulebook_content:\n",
    "    text = content\n",
    "\n",
    "    # Regular expression to find words between '\\n' and '\\n'\n",
    "    pattern = r'(?<=\\n)([A-Z].{2,}[^0-9\\s\\n.])(?=\\n)'\n",
    "\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Filter sentences with 4 or fewer words\n",
    "    filtered_sentences = [match for match in matches if len(match.split()) <= 4]\n",
    "    separators.append(filtered_sentences)\n",
    "    #print(filtered_sentences)\n",
    "\n",
    "\"\"\"\n",
    "# Create a histogram of word lengths\n",
    "plt.hist(len(all_words), bins=range(1, max(len(all_words)) + 2), edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Lengths Between \\\\n Patterns')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separators = r'(?<=\\n)([A-Z].*?)(?=\\n)'\n",
    "chunks = []\n",
    "small_chunks = []\n",
    "\n",
    "for idx, content in enumerate(wh_rulebook_content):\n",
    "    if separators[idx] == []: # add whole page if no separators found\n",
    "        chunks.append(content)\n",
    "        continue\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=separators[idx],\n",
    "        is_separator_regex=False,\n",
    "        keep_separator=True\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.split_text(content)\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        if len(words) < 6 and chunks:  \n",
    "            chunks[-1] += \" \" + text\n",
    "            small_chunks.append(words)\n",
    "        else:\n",
    "            chunks.append(text)\n",
    "    '''\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f'\\nINDEX: {i} - {text}')\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count\n",
      "4       2\n",
      "6       8\n",
      "7      20\n",
      "8      24\n",
      "9      17\n",
      "       ..\n",
      "688     1\n",
      "702     1\n",
      "736     1\n",
      "755     1\n",
      "814     1\n",
      "Name: count, Length: 320, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "words_number = [len(text.split()) for text in chunks]\n",
    "words_count = pd.DataFrame(words_number, columns=['word_count'])\n",
    "\n",
    "# count how many chunks have the same length\n",
    "length_distribution = words_count['word_count'].value_counts().sort_index()\n",
    "\n",
    "print(length_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a histogram of word lengths\n",
    "words_number = [len(text.split()) for text in chunks]\n",
    "plt.hist(words_number, bins=100, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Lengths Between \\\\n Patterns')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Człowiek Krasnolud Elf Niziołek\\n15+k10 15+10k10 30+10k10 15+5k10\\nGENERATOR IMION ELFÓW\\nJeśli podczas wymyślania imienia dla twojej elfiej postaci napotykasz trudności, rzuć kością i porównaj wynik z poniższą tabelą. \\nNiektóre elfy używają wyłącznie dwóch członów w imionach, inne z kolei mogą mieć trzy lub więcej sylab. Rzucaj kośćmi, dopóki \\nnie stworzysz czegoś, co ci odpowiada. Możesz też po prostu wybrać części i złożyć je w całość, która według ciebie brzmi najlepiej.\\n1k10 Pierwsza sylaba Druga sylaba Końcówka (wysokie elfy) Końcówka (leśne elfy)\\n1 Aes a andril arha\\n2 Ath ath anel anhu\\n3 Dor dia ellion dda\\n4 Far en fin han\\n5 Gal for il loc\\n6 Im lor irian noc\\n7 Lin mar mor oth\\n8 Mal ol nil ryn\\n9 Mor sor ric stra\\n10 Ullia than wing wyth\\nIIPOSTAĆ WARHAMMER FANTASY ROLEPLAY\\n40'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"warhammer fantasy\", persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Initialize ChromaDB vector store\"\"\"\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # Create or get collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, chunks, embeddings):\n",
    "        \"\"\"Add document chunks to vector store\"\"\"\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "        \n",
    "        self.collection.add(\n",
    "            documents=[chunk for chunk in chunks],\n",
    "            embeddings=embeddings.tolist(),\n",
    "            ids=ids\n",
    "        )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_dict=None):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=k,\n",
    "            where=filter_dict\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Additional **optional** steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid retriever\n",
    "\n",
    "Combine cos similarity method with additional one which check words frequency in our query(BM25) to choose best available chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - additional retrievel to improve our performance\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_store, embedding_model):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_model = embedding_model\n",
    "        self.bm25 = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def setup_bm25(self, documents):\n",
    "        \"\"\"Setup BM25 for keyword-based retrieval\"\"\"\n",
    "        self.documents = documents\n",
    "        tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Simple tokenization for BM25\"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    def retrieve(self, query, k=10, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining semantic and keyword search\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            k: Number of documents to retrieve\n",
    "            alpha: Weight for semantic search (1-alpha for BM25)\n",
    "        \"\"\"\n",
    "        # Semantic search\n",
    "        query = \"Zapytanie: {query}\"\n",
    "        query_embedding = self.embedding_model.encode(query, show_progress_bar=False)\n",
    "        semantic_results = self.vector_store.similarity_search(query_embedding, k=k*2)\n",
    "        \n",
    "        # BM25 search\n",
    "        if self.bm25:\n",
    "            tokenized_query = self._tokenize(query)\n",
    "            bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "            bm25_results = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:k*2]\n",
    "        else:\n",
    "            bm25_results = []\n",
    "        \n",
    "        # Combine and rerank results\n",
    "        combined_results = self._combine_results(semantic_results, bm25_results, alpha)\n",
    "        \n",
    "        return combined_results[:k]\n",
    "    \n",
    "    def _combine_results(self, semantic_results, bm25_results, alpha):\n",
    "        \"\"\"\n",
    "        Combine semantic and BM25 results with weighted scoring using document index.\n",
    "\n",
    "        Args:\n",
    "            semantic_results: List of document objects from vector store\n",
    "            bm25_results: List of (index, score) tuples from BM25\n",
    "            alpha: Weight for semantic scores\n",
    "\n",
    "        Returns:\n",
    "            List of combined documents reranked by hybrid score\n",
    "        \"\"\"\n",
    "        combined_scores = defaultdict(float)\n",
    "\n",
    "        # Normalize BM25 scores\n",
    "        if bm25_results:\n",
    "            bm25_values = np.array([score for _, score in bm25_results])\n",
    "            bm25_norm = (bm25_values - bm25_values.min()) / (bm25_values.ptp() + 1e-9)\n",
    "            for (idx, _), score in zip(bm25_results, bm25_norm):\n",
    "                combined_scores[idx] += (1 - alpha) * score\n",
    "\n",
    "        # Normalize semantic scores\n",
    "        documents = semantic_results[\"documents\"]\n",
    "        semantic_values = np.array([doc.score if hasattr(doc, \"score\") else 1.0 for doc in documents])\n",
    "        semantic_norm = (semantic_values - semantic_values.min()) / (semantic_values.ptp() + 1e-9)\n",
    "        for idx, (doc, score) in enumerate(zip(documents, semantic_norm)):\n",
    "            combined_scores[idx] += alpha * score\n",
    "\n",
    "        # Rerank based on combined scores\n",
    "        ranked_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return document objects using indices from semantic_results[\"documents\"]\n",
    "        combined_docs = [documents[idx] for idx, _ in ranked_indices if idx < len(documents)]\n",
    "        flattened_docs = [item for sublist in combined_docs for item in sublist]\n",
    "        return flattened_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wh_rulebook_overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3906013052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Okapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwh_rulebook_overlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwh_rulebook_overlap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbm25_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbm25_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm25_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbm25_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wh_rulebook_overlap' is not defined"
     ]
    }
   ],
   "source": [
    "bm = BM25Okapi(wh_rulebook_overlap)\n",
    "test = wh_rulebook_overlap[25]\n",
    "bm25_scores = bm.get_scores(test)\n",
    "bm25_results = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:10*2]\n",
    "bm25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Cerebras(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=cerebras_key\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"What is the meaning of life?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b\",\n",
    "    stream=True,\n",
    "    max_completion_tokens=10240,\n",
    "    temperature=0.7,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Tests\n",
    "# gemini - 6.5\n",
    "# kimi - 5.4\n",
    "# R1 - 21.8\n",
    "# V3 - 12.7\n",
    "# chimera - 35.6\n",
    "# qwen - 29.2\n",
    "\n",
    "models = {'r1':'deepseek/deepseek-r1-0528:free',\n",
    "         'v3':'deepseek/deepseek-chat-v3-0324:free',\n",
    "         'chimera':'tngtech/deepseek-r1t2-chimera:free',\n",
    "         'kimi':'moonshotai/kimi-k2:free',\n",
    "         'gemini':'google/gemini-2.0-flash-exp:free',\n",
    "         'qwen': 'qwen/qwq-32b:free'\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=openrouter_key,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
    "  },\n",
    "  extra_body={},\n",
    "  model = models['qwen'],\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the meaning of life?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wh_rulebook_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3498055118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pl_core_news_lg\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or \"md\", \"lg\" for larger models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwh_rulebook_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcombined_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wh_rulebook_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# split our text into max size chunks\n",
    "def chunk_text(text, chunk_size=100000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# check for entities in our doc\n",
    "nlp = spacy.load(\"pl_core_news_lg\")  # or \"md\", \"lg\" for larger models\n",
    "doc_path = '/kaggle/input/warhammer-4e-rpg/WFRP_4_ed_PL_1.3.pdf'\n",
    "wh_rulebook_raw = ' '.join(load_document(doc_path))\n",
    "\n",
    "chunks = chunk_text(wh_rulebook_raw)\n",
    "docs = [nlp(chunk) for chunk in chunks]\n",
    "combined_doc = Doc.from_docs(docs)\n",
    "entities = [(ent.text, ent.label_) for ent in combined_doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query enchancer\n",
    "\n",
    "Improve our question by generating additional querys which will help model to answer our question.  \n",
    "Additionaly it check if some popular entities are not present in our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEnhancer:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Cerebras(\n",
    "        api_key=api_key\n",
    "        )\n",
    "    \n",
    "    def expand_warhammer_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate multiple query variations for better retrieval\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Biorąc pod uwagę zapytanie dotyczące Warhammer Fantasy: '{query}'\n",
    "        \n",
    "        Wygeneruj 3 alternatywne sformułowania, które mogą pomóc w znalezieniu odpowiednich informacji:\n",
    "        1. Bardziej szczegółowa wersja z terminologią Warhammer Fantasy\n",
    "        2. Szersza wersja obejmująca powiązane pojęcia\n",
    "        3. Wersja skupiająca się na zasadach/mechanice, jeśli ma to zastosowanie\n",
    "        \n",
    "        Zwróć tylko 3 zapytania, po jednym w każdym wierszu.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "        extra_body={},\n",
    "        model = 'llama-3.3-70b',\n",
    "        max_completion_tokens=10240,\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        variations = response.choices[0].message.content.strip().split('\\n')\n",
    "        clean_variations = [v.strip() for v in variations if v.strip()]\n",
    "        return query + ' ' + ' '.join(clean_variations)\n",
    "    \n",
    "    def extract_entities(self, query: str) -> dict:\n",
    "        \"\"\"Extract Warhammer 40k entities from query\"\"\"\n",
    "        warhammer_entities = {\n",
    "            'factions': ['space marines', 'orks', 'eldar', 'tau', 'necrons', 'chaos'],\n",
    "            'unit_types': ['infantry', 'vehicle', 'monster', 'character'],\n",
    "            'weapons': ['bolter', 'lasgun', 'plasma', 'melta'],\n",
    "            'rules': ['armor save', 'weapon skill', 'ballistic skill']\n",
    "        }\n",
    "        \n",
    "        found_entities = {}\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for category, entities in warhammer_entities.items():\n",
    "            found = [entity for entity in entities if entity in query_lower]\n",
    "            if found:\n",
    "                found_entities[category] = found\n",
    "        \n",
    "        return found_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context ranker\n",
    "\n",
    "This component re-ranks retrieved documents in RAG using a cross-encoder.  \n",
    "It evaluates each (query, document) pair together for relevance,  \n",
    "scoring them based on semantic fit, not just keyword overlap.  \n",
    "The top-ranked contexts are then passed to the language model for better answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRanker:\n",
    "    def __init__(self, model_name=\"radlab/polish-cross-encoder\"):\n",
    "        \"\"\"Initialize cross-encoder for reranking\"\"\"\n",
    "        self.reranker = CrossEncoder(model_name, max_length=512)\n",
    "    \n",
    "    def rerank_contexts(self, query: str, contexts: List[str], top_k: int = 5) -> List[dict]:\n",
    "        \"\"\"Rerank retrieved contexts using cross-encoder\"\"\"\n",
    "        # Create query-context pairs\n",
    "        pairs = [(query, context) for context in contexts]\n",
    "        \n",
    "        # Get relevance scores\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort by score and return top_k\n",
    "        scored_contexts = [\n",
    "            {'context': context, 'score': score}\n",
    "            for context, score in zip(contexts, scores)\n",
    "        ]\n",
    "        \n",
    "        return sorted(scored_contexts, key=lambda x: x['score'], reverse=True)[:top_k]\n",
    "    \n",
    "    def filter_by_relevance(self, ranked_contexts: List[dict], threshold: float = 0.3) -> List[dict]:\n",
    "        \"\"\"Filter contexts by relevance threshold\"\"\"\n",
    "        return [ctx for ctx in ranked_contexts if ctx['score'] > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarhammerRAG:\n",
    "    def __init__(self, retriever, ranker, llm_client):\n",
    "        self.retriever = retriever\n",
    "        self.ranker = ranker\n",
    "        self.llm_client = llm_client\n",
    "    \n",
    "    def generate_response(self, query: str, max_context_length: int = 9024) -> dict:\n",
    "        \"\"\"Generate response using RAG pipeline\"\"\"\n",
    "        \n",
    "        # 1. Retrieve relevant contexts\n",
    "        retrieved_contexts = self.retriever.retrieve(query, k=10)\n",
    "        \n",
    "        # 2. Rerank contexts\n",
    "        context_texts = [ctx for ctx in retrieved_contexts]\n",
    "        ranked_contexts = self.ranker.rerank_contexts(query, context_texts, top_k=5)\n",
    "        \n",
    "        # 3. Select contexts within token limit\n",
    "        selected_contexts = self._select_contexts_by_length(ranked_contexts, max_context_length)\n",
    "        \n",
    "        # 4. Generate response\n",
    "        response = self._generate_with_context(query, selected_contexts)\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': selected_contexts,\n",
    "            'retrieved_count': len(retrieved_contexts)\n",
    "        }\n",
    "    \n",
    "    def _select_contexts_by_length(self, contexts: List[dict], max_length: int) -> List[dict]:\n",
    "        \"\"\"Select contexts that fit within token limit\"\"\"\n",
    "        selected = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for ctx in contexts:\n",
    "            ctx_length = len(ctx['context'])  # Simplified length calculation\n",
    "            if current_length + ctx_length <= max_length:\n",
    "                selected.append(ctx)\n",
    "                current_length += ctx_length\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def _generate_with_context(self, query: str, contexts: List[dict]) -> str:\n",
    "        \"\"\"Generate response using LLM with retrieved context\"\"\"\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([f\"[Source {i+1}]: {ctx['context']}\" \n",
    "                                   for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Jesteś asystentem eksperta ds. Warhammera fantasy. Wykorzystaj podany kontekst, aby dokładnie odpowiedzieć na pytanie użytkownika.\n",
    "\n",
    "        Kontekst z materiałów fantastyki Warhammer: {context_text}\n",
    "        Pytanie użytkownika: {query}\n",
    "\n",
    "        Instrukcje:\n",
    "        - Opieraj swoją odpowiedź przede wszystkim na podanym kontekście\n",
    "        - Jeśli kontekst nie zawiera wystarczających informacji, powiedz to jasno\n",
    "        - Używaj odpowiedniej terminologii Warhammer Fantasy\n",
    "        - W razie potrzeby uwzględnij odpowiednie zasady, statystyki lub wiedzę\n",
    "        - Podaj źródła, do których się odwołujesz (np. „Według źródła 1...”)\n",
    "\n",
    "        Odpowiedź:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            extra_body={},\n",
    "            model = 'llama-3.3-70b',\n",
    "            max_completion_tokens=10240,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "            messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aee163063894fcc8009af23d4e4c3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1450a5ccb7664e49b0b4d14d8ccb7f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/127 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f23f9589c94b5e8530813c75ea6721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729c5b16485342ad9b8abf51542da002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7afb9d0e23413a98c9b9f127305b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a4a54ba9f34734b6e87c039827314e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58017a899b0e495d890c6f81c4cda49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9db1cf4e06454f914c2d86d6bbd78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040e0aa4b544575be0d473ef0c8bd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f81430297d940efaea238f9fa7b6c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/673261768.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  wh_embedings = torch.tensor(wh_embedings, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedings ready\n",
      "Vector store ready\n",
      "Retriever initialized\n",
      "Enhanced query generated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7500b1bd14b44b30bdcde452f0fb9829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e408c8fa9f4671a88a87bb028aef6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1632118dcda641dd9f393c2295fff260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3873c17cf34eb2a7492b61687d503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021cc449adb048399d31ae4bec01148b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecada7d30a7455f89d5b0b0d31ece25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker initlialized\n",
      "LLM Loaded\n",
      "RAG ready\n"
     ]
    }
   ],
   "source": [
    "# define our querry\n",
    "query = 'Czy w grze RPG warhammer fantasy 4e mogę korzystać z dwóch kusz jednocześnie?'\n",
    "\n",
    "# load document\n",
    "#document_loader = Document_Loader(start_pg=6, end_pg=-10)\n",
    "#wh_content_chunked = document_loader.smart_overlap_pages()\n",
    "wh_content_chunked = chunks\n",
    "\n",
    "# initialize our embedding model and create emebeddings for our document\n",
    "embeding_creator = Embeding_Creator()\n",
    "embeding_model = embeding_creator.model\n",
    "wh_embedings = embeding_creator.create_embedings(wh_content_chunked)\n",
    "\n",
    "# create our vector database and add our document into it\n",
    "vector_store = VectorStore(\"warhammer_fantasy\")\n",
    "vector_store.add_documents(wh_content_chunked, wh_embedings)\n",
    "print(\"Vector store ready\")\n",
    "\n",
    "# Initialize our hybrid retriever for better content picking\n",
    "retriever = HybridRetriever(vector_store, embeding_model)\n",
    "retriever.setup_bm25(wh_content_chunked)\n",
    "print(\"Retriever initialized\")\n",
    "\n",
    "# generate additional context for our RAG\n",
    "Enhancer = QueryEnhancer(api_key=cerebras_key)\n",
    "query_enhanced = Enhancer.expand_warhammer_query(query)\n",
    "print(\"Enhanced query generated\")\n",
    "\n",
    "# 6. Setup reranker\n",
    "ranker = ContextRanker()\n",
    "print(\"Ranker initlialized\")\n",
    "\n",
    "# 7. Setup LLM client\n",
    "llm_client = Cerebras(api_key=cerebras_key)\n",
    "print(\"LLM Loaded\")\n",
    "\n",
    "# 8. Create RAG system\n",
    "rag_system = WarhammerRAG(retriever, ranker, llm_client)\n",
    "print(\"RAG ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2280575719b4b909faf2f7a4ab5800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'Według podanego kontekstu, nie ma wyraźnych informacji, które wprost regulowałyby używanie dwóch kusz jednocześnie w grze Warhammer Fantasy 4e. Źródła 1-5 dostarczają informacje na temat różnych aspektów gry, takich jak modyfikatory szybkości, zwinności i zmęczenia podróżą, premie do testów broni białej, wyposażenie postaci, rodzaje broni i ich cechy, oraz broń zasięgowa, ale nie odnoszą się bezpośrednio do kwestii używania dwóch kusz jednocześnie.\\n\\nWedług źródła 5, które wymienia broń zasięgową, można wnioskować, że kusze są bronią zasięgową, ale brak informacji na temat używania ich w parach. Źródło 2 wspomina o parowaniu ataków, co sugeruje, że gra posiada mechanizmy dotyczące obrony i ripost, ale nie dostarcza wskazówek co do używania dwóch kusz.\\n\\nPonieważ kontekst nie zawiera wystarczających informacji, aby udzielić precyzyjnej odpowiedzi, trudno jest stwierdzić, czy w Warhammer Fantasy 4e można używać dwóch kusz jednocześnie, jakie są ograniczenia lub zalety takiego działania, oraz jak wpłynęłoby to na parametry walki. Zalecane byłoby skonsultowanie się z pełnymi zasadami gry lub dodatkowymi materiałami, aby uzyskać dokładniejsze informacje na ten temat.',\n",
       " 'sources': [{'context': 'Do podwojo-\\nnego limitu\\nSzybkość –1 (min.: 3), Zwinność –10, \\nZmęczenie Podróżą +1',\n",
       "   'score': 0.69889474},\n",
       "  {'context': 'Parująca przeznaczona jest do parowania ataków. Jeśli \\nposługujesz się taką bronią, dostajesz premię +1 PS do każdego \\nTestu Broni Białej, gdy parujesz atak.',\n",
       "   'score': 0.5750098},\n",
       "  {'context': 'Wyposażenie: relikwia\\n Piewca Zagłady – Brąz 0',\n",
       "   'score': 0.5479925},\n",
       "  {'context': 'Przebijająca\\nAby ustalić Obrażenia zadane na skutek trafienia bronią Przebijającą, \\nmożna wykorzystać wynik rzutu kostką jedności albo PS, cokolwiek \\njest wyższe. Na przykład, jeśli wyrzucisz 34 w Teście ataku, a wynik \\ncelu wynosił 52, możesz użyć PS, który w tym przypadku jest równy \\n2, albo wyniku rzutu jedności, czyli 4. Broń Tępa nigdy nie może być \\ntakże Przebijającą (cecha Tępa jest nadrzędna).',\n",
       "   'score': 0.5412947},\n",
       "  {'context': 'BROŃ ZASIĘGOWA\\nBroń Cena Obc. Dostęp. Zasięg Obrażenia Zalety i Wady',\n",
       "   'score': 0.46451235}],\n",
       " 'retrieved_count': 10}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rag_system = WarhammerRAG(retriever, ranker, llm_client)\n",
    "response = rag_system.generate_response(query_enhanced)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector similarities and bm25 comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOŚWIADCZENIE',\n",
       " 'SCHEMAT CHORÓB',\n",
       " 'Ulicznik',\n",
       " 'MIEJSCE TRAFIENIA',\n",
       " 'Dowodzenia',\n",
       " 'Obieżyświat',\n",
       " 'Wstrzemięźliwy',\n",
       " 'Wstrzemięźliwy',\n",
       " 'Majętny',\n",
       " 'Sianie zamętu']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_test = HybridRetriever(vector_store, embeding_model)\n",
    "retriever_test.setup_bm25(wh_content_chunked)\n",
    "results = retriever_test.retrieve(query, k=10, alpha=0.5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# check best separators (chapters?)\n",
    "# chunks are to big and don't fit into our ranker (remove stop words, bigger ranker, smaller chunks)\n",
    "# relevant scores are pretty bad, how to improve them\n",
    "# fine tunning \"sdadas/mmlw-roberta-base\" embeding model and \"radlab/polish-cross-encoder\" ranker model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7420642,
     "sourceId": 11814496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
